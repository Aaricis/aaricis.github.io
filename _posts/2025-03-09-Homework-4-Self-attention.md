---
title: Self-attention
date: 2025-03-09 15:07:00 +/-8
categories: [Machine Learning 2022]
tags: [Hung-yi Lee]     # TAG names should always be lowercase
---

## Objective

- äº†è§£Self-attentionï¼›

- å­¦ä¼šä½¿ç”¨Transformerã€‚

## Task Description

Speaker Identificationï¼ˆè¯­è€…è¯†åˆ«ï¼‰ï¼šå¤šåˆ†ç±»ä»»åŠ¡ï¼Œä»ç»™å®šçš„è¯­éŸ³ä¸­é¢„æµ‹æ¼”è®²è€…çš„ç±»åˆ«ã€‚

## Dataset 

VoxCeleb2

- Training: 56666 processed audio features with labels. 
- Testing: 4000 processed audio features (public & private) without labels. 
- Label: 600 classes in total, each class represents a speaker.

## æ€è·¯

### Simple Baseline(> 0.60824)
ç›´æ¥è·‘ä¸€éSimple Codeï¼ˆScore: 0.60500
Private score: 0.61300ï¼‰ä¸èƒ½è¾¾åˆ°ã€‚

Score: 0.65375
Private score: 0.65625

æ¨¡å‹ä½¿ç”¨ä¸¤å±‚TransformerğŸ‘‡
```python
self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2)
```
forwardä¸­ä½¿ç”¨encoder
```python
# out = self.encoder_layer(out)
out = self.encoder(out)
```

### Medium Baseline(> 0.70375)
Score: 0.76825
Private score: 0.76925

- å¢åŠ `d_model`çš„å€¼ï¼Œå› ä¸ºè¾“å‡ºç»´åº¦n_spks=600å·®è·è¿‡å¤§ï¼›
```python
d_model=224
```
- `nn.TransformerEncoderLayer`å¢åŠ `dropout`å‚æ•°ï¼Œ`dropout=0.2`ï¼›
```python
self.encoder_layer = nn.TransformerEncoderLayer(
			d_model=d_model, dim_feedforward=256, nhead=2, dropout=dropout
		)
```
- å †å 3ä¸ªtransformerå±‚ï¼›
```python
self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=3)
```
- è¾“å‡ºå±‚æ”¹ä¸ºä¸€å±‚å¹¶å¢åŠ batchnormæœºåˆ¶ï¼›
```python
self.pred_layer = nn.Sequential(
      nn.BatchNorm1d(d_model),
      nn.Linear(d_model, n_spks),
    )
```

### Strong Baseline(>0.77750)
Score: 0.85025
Private score: 0.85025

ä½¿ç”¨ `torchaudio.models.Conformer` ä½œä¸ºç¼–ç å™¨ä»£æ›¿Transformerï¼Œæå–åºåˆ—ç‰¹å¾ã€‚
ref:https://pytorch.org/audio/stable/generated/torchaudio.models.Conformer.html

æ³¨æ„:
- Conformerçš„è¾“å…¥å½¢çŠ¶ä¸º(batch_size, seq_len, d_model)ï¼Œè¾“å‡ºå½¢çŠ¶ç›¸åŒï¼›
- `Conformer.forward()`éœ€è¦é¢å¤–è¾“å…¥`lengths`,æ˜¯ä¸€ä¸ªå¼ é‡ï¼Œè¡¨ç¤ºæ¯ä¸ªè¾“å…¥åºåˆ—çš„å®é™…é•¿åº¦ï¼Œä»¥ä¾¿æ¨¡å‹åœ¨å¤„ç†æ—¶å¯ä»¥å¿½ç•¥å¡«å……éƒ¨åˆ†ï¼ˆpaddingï¼‰ï¼›
```python
lengths = torch.full((out.shape[0],),out.shape[1])
```
  (1)out.shape: out æ˜¯ç»è¿‡ prenet å¤„ç†åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len, d_model)ã€‚

  - batch_sizeï¼šæ‰¹æ¬¡å¤§å°ã€‚

  - seq_lenï¼šåºåˆ—é•¿åº¦ï¼ˆæ—¶é—´æ­¥æ•°ï¼‰ã€‚

  - d_modelï¼šç‰¹å¾ç»´åº¦ã€‚

  - out.shape[0] æ˜¯æ‰¹æ¬¡å¤§å°ï¼ˆbatch_sizeï¼‰ã€‚

  - out.shape[1] æ˜¯åºåˆ—é•¿åº¦ï¼ˆseq_lenï¼‰ã€‚

  (2)torch.full: torch.full æ˜¯ PyTorch ä¸­çš„ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåˆ›å»ºä¸€ä¸ªå¡«å……äº†æŒ‡å®šå€¼çš„å¼ é‡ã€‚

  è¯­æ³•ï¼štorch.full(size, fill_value, ...)ã€‚

  sizeï¼šå¼ é‡çš„å½¢çŠ¶ã€‚

  fill_valueï¼šå¡«å……çš„å€¼ã€‚

  (3) (out.shape[0],):
  - è¿™æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œè¡¨ç¤ºå¼ é‡çš„å½¢çŠ¶ã€‚
  - (out.shape[0],) è¡¨ç¤ºä¸€ä¸ªä¸€ç»´å¼ é‡ï¼Œé•¿åº¦ä¸º batch_sizeã€‚

  (4) out.shape[1]:
  è¿™æ˜¯å¡«å……çš„å€¼ï¼Œè¡¨ç¤ºåºåˆ—çš„é•¿åº¦ï¼ˆseq_lenï¼‰ã€‚

```python
import torch
import torch.nn as nn
import torchaudio.models

class Classifier(nn.Module):
    def __init__(self, d_model=224, n_spks=600, dropout=0.2):
        super().__init__()
        # å°†è¾“å…¥ç»´åº¦ä» 40 æ˜ å°„åˆ° d_model
        self.prenet = nn.Linear(40, d_model)

        # ä½¿ç”¨ Conformer ä»£æ›¿ Transformer
        self.encoder = torchaudio.models.Conformer(
            input_dim=d_model,  # è¾“å…¥ç‰¹å¾ç»´åº¦
            num_heads=2,  # æ³¨æ„åŠ›å¤´æ•°
            ffn_dim=256,  # Feed-forward å±‚éšè—ç»´åº¦
            num_layers=3,  # Conformer å±‚æ•°
            depthwise_conv_kernel_size=31,  # æ·±åº¦å·ç§¯æ ¸å¤§å°
            dropout=dropout,
        )

        # é¢„æµ‹å±‚ï¼ˆBatchNorm + çº¿æ€§åˆ†ç±»å±‚ï¼‰
        self.pred_layer = nn.Sequential(
            nn.BatchNorm1d(d_model),
            nn.Linear(d_model, n_spks),
        )

    def forward(self, mels):
        """
        args:
            mels: (batch_size, seq_len, 40)
        return:
            out: (batch_size, n_spks)
        """
        # çº¿æ€§å˜æ¢è°ƒæ•´ç»´åº¦
        out = self.prenet(mels)  # (batch_size, seq_len, d_model)

        # Conformer æœŸæœ›è¾“å…¥æ ¼å¼ (batch_size, seq_len, d_model)ï¼Œæ— éœ€ permute
        lengths = torch.full((out.shape[0],),out.shape[1]).to(device)
        out, _ = self.encoder(out, lengths)

        # Mean Pooling å–å…¨å±€ä¿¡æ¯
        stats = out.mean(dim=1)  # (batch_size, d_model)

        # é€šè¿‡åˆ†ç±»å±‚
        out = self.pred_layer(stats)  # (batch_size, n_spks)

        return out

```

### Boss Baseline(>0.86500)
Score: 0.87575
Private score: 0.87250

- æ ¹æ®åŠ©æ•™æç¤ºï¼Œç›´æ¥åŠ ä¸ŠSelf-Attention Poolingå’ŒAdditive Margin Softmax,è®­ç»ƒ15ä¸‡ä¸ªstepï¼Œç»“æœå°±ç›´æ¥æƒ¨æ‰ï¼ˆScore: 0.83900 Private score: 0.83900ï¼‰ï¼Œæ¯”Strong Baselineçš„scoreè¿˜ä¸å¦‚ã€‚

- åªä½¿ç”¨Conformer+Self-Attention Poolingï¼Œç»“æœç•¥æœ‰æå‡ï¼Œä½†æ˜¯è¿˜ä¸èƒ½è¾¾åˆ°Boss Baseline(Score: 0.85550 Private score: 0.85550)ã€‚Additive Margin Softmaxçœ‹èµ·æ¥å®é™…æ²¡æœ‰èµ·åˆ°ä»€ä¹ˆä½œç”¨ã€‚

- è°ƒæ•´Conformerå¤´æ•°ã€å±‚æ•°, dropoutç­‰å‚æ•°ï¼Œscoreç•¥æœ‰æ³¢åŠ¨ï¼Œä½†ä¸æ˜æ˜¾ã€‚

- æ¨¡å‹å¢åŠ BatchNormå±‚ã€l2æ­£åˆ™ï¼Œæ”¹è¿›Poolingå±‚ï¼ˆMean + Self-Attentionï¼‰è®©æ± åŒ–æ›´ç¨³å®šï¼Œscoreç•¥æœ‰å¢åŠ ã€‚

è‡³æ­¤ï¼Œå•æ¨¡è¿˜æ˜¯ä¸èƒ½è¾¾åˆ°Boss Baselineï¼Œè·ŸåŠ©æ•™çš„è¯´æ³•ä¸ä¸€æ ·ï¼Œä¸çŸ¥é“æ˜¯ä»£ç è¿˜æ˜¯å‚æ•°çš„é—®é¢˜ï¼Œå¦‚æœèƒ½æœ‰æ”¹è¿›çš„å»ºè®®ï¼Œéå¸¸æ„Ÿè°¢ : )

æœ€ç»ˆåªå¥½ä½¿ç”¨æŠ•ç¥¨æ³•Ensembleï¼Œæ‰è¶…è¿‡Boss Baselineã€‚

- **Self-Attention Pooling**

```python

class SelfAttentionPooling(nn.Module):
    """
    Mean Pooling + Self-Attention Pooling
    """
    def __init__(self, input_dim):
        super(SelfAttentionPooling, self).__init__()
        self.attention = nn.Linear(input_dim, 1)

    def forward(self, batch_rep):
        """
        input:
            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension

        return:
            utter_rep: size (N, H)
        """
        # Self-Attention Weights
        att_w = F.softmax(self.attention(batch_rep), dim=1)  # (N, T, 1)
        att_out = torch.sum(batch_rep * att_w, dim=1)  # Weighted sum (N, H)
        
        # Combine both
        utter_rep = att_out
        return utter_rep

```

- **Additive Margin Softmax**

```python

class AMSoftmax(nn.Module):
    def __init__(self, in_features, n_classes, s=20, m=0.2): # s=30, m=0.4
        super(AMSoftmax, self).__init__()
        self.in_features = in_features
        self.n_classes = n_classes
        self.s = s  # ç¼©æ”¾å› å­
        self.m = m  # è§’åº¦è¾¹è·
        self.weight = nn.Parameter(torch.FloatTensor(n_classes, in_features))
        nn.init.xavier_uniform_(self.weight)  # åˆå§‹åŒ–æƒé‡

    def forward(self, x, labels=None):
        """
        x: (batch_size, in_features) è¾“å…¥ç‰¹å¾
        labels: (batch_size,) ç›®æ ‡æ ‡ç­¾ (è®­ç»ƒæ¨¡å¼) æˆ– None (æ¨ç†æ¨¡å¼)
        return: (batch_size, n_classes) AM-Softmax logits
        """
        # å½’ä¸€åŒ–è¾“å…¥å’Œæƒé‡
        x = F.normalize(x, p=2, dim=-1)  # å½’ä¸€åŒ–è¾“å…¥ç‰¹å¾
        weight = F.normalize(self.weight, p=2, dim=-1)  # å½’ä¸€åŒ–æƒé‡

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        cosine = F.linear(x, weight)  # (batch_size, n_classes)

        if labels is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®— AM-Softmax
            one_hot = F.one_hot(labels.to(torch.int64), num_classes=self.n_classes).float()
            cosine_m = cosine - self.m * one_hot  # ä»…å¯¹ç›®æ ‡ç±»æ–½åŠ  margin
            logits = self.s * cosine_m
        else:
            # æ¨ç†æ¨¡å¼ï¼šä»…è¿›è¡Œå½’ä¸€åŒ–è®¡ç®—
            logits = self.s * cosine  # ç›´æ¥è¿”å› softmax logits

        return logits

```

## Code

[åŒè¿‡Boss Baseline](https://github.com/Aaricis/Hung-yi-Lee-ML2022/blob/main/HW4/hw04-speaker-identification.ipynb)

## Report

**1. Make a brief introduction about a variant of Transformer. (2 pts)** 

Conformeræ˜¯ä¸€ç§ç»“åˆäº†Transformerå’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æ¨¡å‹ï¼Œä¸»è¦ç”¨äºå¤„ç†åºåˆ—æ•°æ®ï¼ˆå¦‚è¯­éŸ³ã€æ–‡æœ¬ç­‰ï¼‰ã€‚å®ƒé€šè¿‡å·§å¦™åœ°èåˆTransformeråœ°è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒCNNåœ°å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›ï¼Œåœ¨è¯­éŸ³è¯†åˆ«ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—æ•ˆæœã€‚

**2. Briefly explain why adding convolutional layers to Transformer can boost performance. (2 pts)**

å…¨å±€ä¸å±€éƒ¨ç‰¹å¾çš„ç»“åˆï¼š

- Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ“…é•¿æ•æ‰å…¨å±€ä¾èµ–å…³ç³»ã€‚
- CNN çš„å·ç§¯æ¨¡å—æ“…é•¿æ•æ‰å±€éƒ¨ç‰¹å¾ã€‚
- Conformer é€šè¿‡ç»“åˆä¸¤è€…ï¼Œèƒ½å¤ŸåŒæ—¶å»ºæ¨¡åºåˆ—ä¸­çš„å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯ã€‚

## Reference

[æå®æ¯…æ·±åº¦å­¦ä¹  2021 ä½œä¸šå›› Self-Attention å®éªŒè®°å½• - Niku's Blog](https://www.nikunokoya.com/posts/lhy2021_hw4/#conformer)

