---
title: Intro to Large Language Models
date: 2024-08-25 16:40:00 +/-8
categories: [LLM]
tags: [Hung-yi Lee]     # TAG names should always be lowercase
---

# Intro to Large Language Models



在讲大语言模型之前首先讲讲什么是语言模型吧！

## 什么是语言模型？

语言模型是一种机器学习模型，旨在预测和生成合理的语言。例如，语句自动补全背后就是语言模型。这些模型用来估计一个token或一系列token在一个较长的token序列中出现的概率。考虑下面的句子：

```text
When I hear rain on my roof, I _______ in my kitchen.
```

假设token是一个单词，语言模型将确定替换该下划线的不同单词或单词序列的概率。例如，语言模型可能确定以下概率:

```
cook soup 9.4%
warm up a kettle 5.2%
cower 3.6%
nap 2.5%
relax 2.2%
...
```

“token序列”可以是一个完整的句子或一系列的句子。也就是说，语言模型可以计算不同的完整句子或文本块的可能性。

估计序列中下一个token的概率对各种任务都很有用:生成文本、翻译语言和回答问题等等。

# 什么是大语言模型？

建模人类语言规模庞大，是一项极为复杂且资源密集的任务。当前语言模型和大型语言模型所具备的能力经过了几十年的发展历程。

随着模型变的越来越大，它们的复杂度和效力也随之增加。早期的语言模型可以预测一个单词的概率；而现代大语言模型可以预测句子、段落甚至整个文档的概率。

在过去几年中，随着计算机内存、数据集大小和处理能力的提升，以及更有效的文本序列建模技术的发展，语言模型的大小和能力也得到了爆炸式的增长。

## 多大才算大？

这个定义是模糊的，但是“大”已经被用来描述BERT (110M个参数)和PaLM 2(高达340B个参数)。

参数是模型在训练过程中学习到的权重，用于预测序列中的下一个标记。“Large”既可以指模型中的参数数量，有时也可以指数据集中的单词数量。

## Transformer

语言建模的一个关键发展是2017年推出的Transformer，这是一种围绕注意力概念设计的架构。这使得通过专注于输入的最重要部分来处理更长的序列成为可能，解决了早期模型中遇到的内存问题。

完整的Transformer由一个encoder和一个decoder组成。encoder将输入文本转换为中间表示，decoder将中间表示转换为有用的文本。

## Self-attention

Transformer很大程度上依赖self-attention。self-attention的self部分指语料库中每个token的 "egocentric" focus。对于每一个输入token，self-attention会问：“其他输入的token对我有多重要？”。假设每个token是一个单词，而完整的上下文是一个句子。考虑下面的句子:

```
The animal didn't cross the street because it was too tired.
```

前面的句子里有11个单词，所以这11个单词中的每一个都在关注另外10个单词，想知道这10个单词对他们有多重要。

# 大语言模型有哪些应用？

LLMs 在它们被设计用来完成的任务上表现得非常出色，即根据输入生成最合理的文本。它们在其他任务上也表现出强大的性能。例如，摘要、问题解答和文本分类。这些被称为“ [emergent abilities](https://research.google/pubs/pub52065/)”。LLMs 甚至可以解决一些数学问题并编写代码。

LLMs非常擅长模仿人类的语言模式，擅长将信息与不同风格和语气结合起来。也可以是构建比仅生成文本更复杂模型，例如情感分析器、毒性分类器，并生成图像说明等。

# Reference

[What is a language model?](https://developers.google.com/machine-learning/resources/intro-llms#what_is_a_language_model)