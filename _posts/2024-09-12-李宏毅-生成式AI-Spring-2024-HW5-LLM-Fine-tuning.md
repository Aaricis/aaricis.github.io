---
title: ã€æå®æ¯…-ç”Ÿæˆå¼AIã€‘Spring 2024, HW5ï¼šLLM Fine-tuning
date: 2024-09-12 16:40:00 +/-8
categories: [LLM, Spring 24 GenAI]
tags: [Hung-yi Lee]     # TAG names should always be lowercase
---

#  Task Overview

è®­ç»ƒä¸€ä¸ªä¼šå†™å”è¯—çš„AIæ¨¡å‹ã€‚ç»™å®šAIæ¨¡å‹è¯—çš„å‰ä¸¤å¥ï¼Œå†™å‡ºè¯—çš„åä¸¤å¥ã€‚

åŸæœ¬çš„LLMä¸å…·å¤‡å†™è¯—çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç”¨ChatGPTå’Œkimiéƒ½è¯•ä¸€ä¸‹ğŸ‘‡ï¼Œå®ƒä»¬æ— ä¸€ä¾‹å¤–éƒ½è¾“å‡ºäº†å¯¹è¯—çš„é‰´èµã€‚

![](../assets/images/Hung-yi_Lee/hw5-1.png)

![](../assets/images/Hung-yi_Lee/hw5-2.png)

æ²¡æœ‰ç»è¿‡Fine-tuningçš„æ¨¡å‹ï¼Œä¸å…·å¤‡å†™å”è¯—çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯æ•™AIæ¨¡å‹å†™å”è¯—ã€‚

![](../assets/images/Hung-yi_Lee/hw5-3.png)





#  Model and Dataset

## Model

å®éªŒæä¾›äº†ä¸¤ä¸ª70äº¿å‚æ•°çš„æ¨¡å‹å¯ä¾›é€‰æ‹©ï¼š

1. Taide-7Bï¼šTaide7Bæ¨¡å‹æ˜¯â€œå¯ä¿¡AIå¯¹è¯å¼•æ“â€ï¼ˆTAIDEï¼‰é¡¹ç›®çš„ä¸€éƒ¨åˆ†ï¼Œä¸»è¦ä¸ºå°æ¹¾å¼€å‘ã€‚è¯¥æ¨¡å‹åŸºäºLLaMaæ¨¡å‹ï¼Œä¸“æ³¨äºå¤„ç†ç¹ä½“ä¸­æ–‡ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¿»è¯‘ã€æ‘˜è¦ã€ä¿¡ä»¶å†™ä½œå’Œæ–‡ç« ç”Ÿæˆã€‚
2. MediaTek Breeze 7Bï¼šMR Breeze-7B æ˜¯è”å‘ç§‘æ——ä¸‹ç ”ç©¶æœºæ„è”å‘ç§‘æŠ€ç ”ç©¶ä¸­å¿ƒï¼ˆMediaTek Researchï¼‰å¼€å‘çš„ä¸€æ¬¾å…¨æ–°å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸“ä¸ºå¤„ç†ç¹ä½“ä¸­æ–‡å’Œè‹±æ–‡è€Œè®¾è®¡ã€‚è¿™æ¬¾æ¨¡å‹æ‹¥æœ‰70äº¿ä¸ªå‚æ•°ï¼ŒåŸºäºå¹¿å—èµèª‰çš„Mistralæ¨¡å‹è¿›è¡Œè®¾è®¡å’Œä¼˜åŒ–ã€‚

## Dataset

ä¸“é—¨ç”¨äºå¾®è°ƒLLMçš„å”è¯—æ•°æ®é›† [Tang poem dataset](https://github.com/CheeEn-Yu/GenAI-Hw5)ï¼Œé‡Œé¢åŒ…å«5000é¦–è¯—ã€‚

![](../assets/images/Hung-yi_Lee/hw5-4.png)

datasetä¸»è¦åŒ…å«ä¸¤ä¸ªJSONæ–‡ä»¶ï¼š

- Tang_testing_data.jsonï¼šæµ‹è¯•é›†ï¼ŒåŒ…å«15æ¡æ•°æ®
- Tang_training_data.jsonï¼šè®­ç»ƒé›†ï¼ŒåŒ…å«5001æ¡æ•°æ®

è®­ç»ƒé›†æ•°æ®å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒåŒ…å«`instruction`, `input`, `output`ï¼›æµ‹è¯•é›†åªåŒ…å«`instruction`, `input`ï¼Œç­”æ¡ˆåœ¨Tang_tesing_gt.txtæ–‡ä»¶ä¸­ã€‚

# Changing the Generation Behavior:Decoding Parameters

ç”Ÿæˆå¼æ¨¡å‹é€‰æ‹©ä¸‹ä¸€ä¸ªtokençš„æ–¹æ³•æ˜¯ï¼šä»ä¸‹ä¸€ä¸ªtokençš„åˆ†å¸ƒä¸­é‡‡æ ·ã€‚

![](../assets/images/Hung-yi_Lee/hw5-5.png)

é€šè¿‡æ”¹å˜é‡‡æ ·æ–¹å¼ï¼Œå¯ä»¥æ”¹å˜è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸‹ä¸€ä¸ªtokençš„æ–¹å¼ã€‚

æˆ‘ä»¬å¯ä»¥è°ƒæ•´æ¨¡å‹è¶…å‚æ•°ï¼Œæ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºã€‚è®©æ¨¡å‹çš„è¾“å‡ºï¼šlonger vs. shorter; diverse vs.staticï¼›è¶…å‚æ•°æœ‰ï¼š

- temperature
- Top-k
- Top-p
- max_length

## Temperature

temperatureæ§åˆ¶æ¨¡å‹è¾“å‡ºçš„diversityã€‚å®ƒæ”¹å˜äº†æ•°æ®çš„åˆ†å¸ƒæ¦‚ç‡ï¼Œtemperatureè¶Šå°ï¼Œæ¨¡å‹çš„è¾“å‡ºè¶Šå›ºå®šï¼›temperatureè¶Šå¤§ï¼Œæ¨¡å‹çš„è¾“å‡ºè¶Šéšæœºï¼Œè¾“å…¥åŒæ ·promptï¼Œæ¨¡å‹çš„è¾“å‡ºå·®å¼‚å¾ˆå¤§ã€‚

![](../assets/images/Hung-yi_Lee/hw5-6.png)

## Top-K

Top-Kè¡¨ç¤ºæ¯æ¬¡é€‰æ‹©å‡ ç‡æœ€é«˜çš„Kä¸ªå­—ï¼Œç„¶ååœ¨Kä¸ªå­—ä¸­ä½œrandom searchï¼Œé€‰æ‹©ä¸€ä¸ªå­—ä½œä¸ºä¸‹ä¸€ä¸ªtokenã€‚

## Top-P

Top-Pè¡¨ç¤ºæ¯æ¬¡é€‰æ‹©å‡ ç‡åŠ èµ·æ¥<pçš„tokenç»„åˆï¼Œç„¶ååœ¨è¯¥ç»„åˆé‡Œé¢ä½œrandom searchã€‚

## Max_length

æ¨¡å‹è¾“å‡ºçš„æœ€å¤§é•¿åº¦ã€‚max_lengthè¿‡å°ï¼Œæ¨¡å‹çš„è¾“å‡ºä¼šè¢«æˆªæ–­ï¼›max_lengthè¿‡å¤§ï¼Œä¼šæ¶ˆè€—è¿‡å¤šçš„è®¡ç®—èµ„æºã€‚

# ä»£ç ä»‹ç»

[code](https://colab.research.google.com/drive/1nB3jwRJVKXSDDNO-pbURrao0N2MpqHl8?usp=sharing&fbclid=IwAR3AeFT3dCW1BED25hPaMJ2AUyZ_N-1vya7Or7LaRKg2uvMeBZdWF7w3yJs)

## Fix Random Seeds

å¾®è°ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¶‰åŠä¸€äº›éšæœºæ€§ã€‚å›ºå®šéšæœºç§å­ï¼Œä½¿ç»“æœå…·æœ‰å¯é‡å¤æ€§ã€‚

```python
seed = 42
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)
```

## åŠ è½½LLM

ä½¿ç”¨transformersåº“çš„AutoModelForCausalLMæ¥åŠ è½½æ¨¡å‹ï¼Œcache_dirä¸ºä¸‹è½½æ¨¡å‹çš„ç›®å½•ã€‚

```python
cache_dir = "./cache"

nf4_config = BitsAndBytesConfig(
   load_in_4bit=True,
   bnb_4bit_quant_type="nf4",
   bnb_4bit_use_double_quant=True,
   bnb_4bit_compute_dtype=torch.bfloat16
)

# å¾æŒ‡å®šçš„æ¨¡å‹åç¨±æˆ–è·¯å¾‘è¼‰å…¥é è¨“ç·´çš„èªè¨€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    cache_dir=cache_dir,
    quantization_config=nf4_config,
    low_cpu_mem_usage = True
)

```

## åŠ è½½tokenizer

åŠ è½½LLMçš„tokenizerã€‚åœ¨LLMä¸­ï¼Œtokenizeræ˜¯ä¸€ä¸ªå…³é”®ç»„ä»¶ï¼Œç”¨äºå°†è¾“å…¥çš„æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—åºåˆ—ã€‚

```python
# å‰µå»º tokenizer ä¸¦è¨­å®šçµæŸç¬¦è™Ÿ (eos_token)
logging.getLogger('transformers').setLevel(logging.ERROR)
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    add_eos_token=True,
    cache_dir=cache_dir,
    quantization_config=nf4_config
)
tokenizer.pad_token = tokenizer.eos_token
```

## è®¾ç½®è§£ç å‚æ•°

è®¾ç½®æ¨¡å‹ä½œinferenceæ—¶çš„decoding parametersï¼›

```python
# è¨­å®šæ¨¡å‹æ¨ç†æ™‚éœ€è¦ç”¨åˆ°çš„decoding parameters
max_len = 128
generation_config = GenerationConfig(
    do_sample=True,
    temperature=0.1,
    num_beams=1,
    top_p=0.3,
    no_repeat_ngram_size=3,
    pad_token_id=2,
)
```

