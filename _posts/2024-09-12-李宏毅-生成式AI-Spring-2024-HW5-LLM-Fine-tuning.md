---
title: ã€æå®æ¯…-ç”Ÿæˆå¼AIã€‘Spring 2024, HW5ï¼šLLM Fine-tuning
date: 2024-09-12 16:40:00 +/-8
categories: [LLM, Spring 24 GenAI]
tags: [Hung-yi Lee]     # TAG names should always be lowercase
---

#  Task Overview

è®­ç»ƒä¸€ä¸ªä¼šå†™å”è¯—çš„AIæ¨¡å‹ã€‚ç»™å®šAIæ¨¡å‹è¯—çš„å‰ä¸¤å¥ï¼Œå†™å‡ºè¯—çš„åä¸¤å¥ã€‚

åŸæœ¬çš„LLMä¸å…·å¤‡å†™è¯—çš„èƒ½åŠ›ã€‚æˆ‘ä»¬ç”¨ChatGPTå’Œkimiéƒ½è¯•ä¸€ä¸‹ğŸ‘‡ï¼Œå®ƒä»¬æ— ä¸€ä¾‹å¤–éƒ½è¾“å‡ºäº†å¯¹è¯—çš„é‰´èµã€‚

![](../assets/images/Hung-yi_Lee/hw5-1.png)

![](../assets/images/Hung-yi_Lee/hw5-2.png)

æ²¡æœ‰ç»è¿‡Fine-tuningçš„æ¨¡å‹ï¼Œä¸å…·å¤‡å†™å”è¯—çš„èƒ½åŠ›ã€‚æˆ‘ä»¬çš„ç›®çš„æ˜¯æ•™AIæ¨¡å‹å†™å”è¯—ã€‚

![](../assets/images/Hung-yi_Lee/hw5-3.png)





#  Model and Dataset

## Model

å®éªŒæä¾›äº†ä¸¤ä¸ª70äº¿å‚æ•°çš„æ¨¡å‹å¯ä¾›é€‰æ‹©ï¼š

1. Taide-7Bï¼šTaide7Bæ¨¡å‹æ˜¯â€œå¯ä¿¡AIå¯¹è¯å¼•æ“â€ï¼ˆTAIDEï¼‰é¡¹ç›®çš„ä¸€éƒ¨åˆ†ï¼Œä¸»è¦ä¸ºå°æ¹¾å¼€å‘ã€‚è¯¥æ¨¡å‹åŸºäºLLaMaæ¨¡å‹ï¼Œä¸“æ³¨äºå¤„ç†ç¹ä½“ä¸­æ–‡ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç¿»è¯‘ã€æ‘˜è¦ã€ä¿¡ä»¶å†™ä½œå’Œæ–‡ç« ç”Ÿæˆã€‚
2. MediaTek Breeze 7Bï¼šMR Breeze-7B æ˜¯è”å‘ç§‘æ——ä¸‹ç ”ç©¶æœºæ„è”å‘ç§‘æŠ€ç ”ç©¶ä¸­å¿ƒï¼ˆMediaTek Researchï¼‰å¼€å‘çš„ä¸€æ¬¾å…¨æ–°å¼€æºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œä¸“ä¸ºå¤„ç†ç¹ä½“ä¸­æ–‡å’Œè‹±æ–‡è€Œè®¾è®¡ã€‚è¿™æ¬¾æ¨¡å‹æ‹¥æœ‰70äº¿ä¸ªå‚æ•°ï¼ŒåŸºäºå¹¿å—èµèª‰çš„Mistralæ¨¡å‹è¿›è¡Œè®¾è®¡å’Œä¼˜åŒ–ã€‚

## Dataset

ä¸“é—¨ç”¨äºå¾®è°ƒLLMçš„å”è¯—æ•°æ®é›† [Tang poem dataset](https://github.com/CheeEn-Yu/GenAI-Hw5)ï¼Œé‡Œé¢åŒ…å«5000é¦–è¯—ã€‚

![](../assets/images/Hung-yi_Lee/hw5-4.png)

datasetä¸»è¦åŒ…å«ä¸¤ä¸ªJSONæ–‡ä»¶ï¼š

- Tang_testing_data.jsonï¼šæµ‹è¯•é›†ï¼ŒåŒ…å«15æ¡æ•°æ®
- Tang_training_data.jsonï¼šè®­ç»ƒé›†ï¼ŒåŒ…å«5001æ¡æ•°æ®

è®­ç»ƒé›†æ•°æ®å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒåŒ…å«`instruction`, `input`, `output`ï¼›æµ‹è¯•é›†åªåŒ…å«`instruction`, `input`ï¼Œç­”æ¡ˆåœ¨Tang_tesing_gt.txtæ–‡ä»¶ä¸­ã€‚

# Changing the Generation Behavior:Decoding Parameters

ç”Ÿæˆå¼æ¨¡å‹é€‰æ‹©ä¸‹ä¸€ä¸ªtokençš„æ–¹æ³•æ˜¯ï¼šä»ä¸‹ä¸€ä¸ªtokençš„åˆ†å¸ƒä¸­é‡‡æ ·ã€‚

![](../assets/images/Hung-yi_Lee/hw5-5.png)

é€šè¿‡æ”¹å˜é‡‡æ ·æ–¹å¼ï¼Œå¯ä»¥æ”¹å˜è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸‹ä¸€ä¸ªtokençš„æ–¹å¼ã€‚

æˆ‘ä»¬å¯ä»¥è°ƒæ•´æ¨¡å‹è¶…å‚æ•°ï¼Œæ§åˆ¶æ¨¡å‹çš„è¡Œä¸ºã€‚è®©æ¨¡å‹çš„è¾“å‡ºï¼šlonger vs. shorter; diverse vs.staticï¼›è¶…å‚æ•°æœ‰ï¼š

- temperature
- Top-k
- Top-p
- max_length

## Temperature

temperatureæ§åˆ¶æ¨¡å‹è¾“å‡ºçš„diversityã€‚å®ƒæ”¹å˜äº†æ•°æ®çš„åˆ†å¸ƒæ¦‚ç‡ï¼Œtemperatureè¶Šå°ï¼Œæ¨¡å‹çš„è¾“å‡ºè¶Šå›ºå®šï¼›temperatureè¶Šå¤§ï¼Œæ¨¡å‹çš„è¾“å‡ºè¶Šéšæœºï¼Œè¾“å…¥åŒæ ·promptï¼Œæ¨¡å‹çš„è¾“å‡ºå·®å¼‚å¾ˆå¤§ã€‚

![](../assets/images/Hung-yi_Lee/hw5-6.png)

## Top-K

Top-Kè¡¨ç¤ºæ¯æ¬¡é€‰æ‹©å‡ ç‡æœ€é«˜çš„Kä¸ªå­—ï¼Œç„¶ååœ¨Kä¸ªå­—ä¸­ä½œrandom searchï¼Œé€‰æ‹©ä¸€ä¸ªå­—ä½œä¸ºä¸‹ä¸€ä¸ªtokenã€‚

## Top-P

Top-Pè¡¨ç¤ºæ¯æ¬¡é€‰æ‹©å‡ ç‡åŠ èµ·æ¥<pçš„tokenç»„åˆï¼Œç„¶ååœ¨è¯¥ç»„åˆé‡Œé¢ä½œrandom searchã€‚

## Max_length

æ¨¡å‹è¾“å‡ºçš„æœ€å¤§é•¿åº¦ã€‚max_lengthè¿‡å°ï¼Œæ¨¡å‹çš„è¾“å‡ºä¼šè¢«æˆªæ–­ï¼›max_lengthè¿‡å¤§ï¼Œä¼šæ¶ˆè€—è¿‡å¤šçš„è®¡ç®—èµ„æºã€‚

# ä»£ç ä»‹ç»

[code](https://colab.research.google.com/drive/1nB3jwRJVKXSDDNO-pbURrao0N2MpqHl8?usp=sharing&fbclid=IwAR3AeFT3dCW1BED25hPaMJ2AUyZ_N-1vya7Or7LaRKg2uvMeBZdWF7w3yJs)

## Fix Random Seeds

å¾®è°ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ¶‰åŠä¸€äº›éšæœºæ€§ã€‚å›ºå®šéšæœºç§å­ï¼Œä½¿ç»“æœå…·æœ‰å¯é‡å¤æ€§ã€‚

```python
seed = 42
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.manual_seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)
```

## åŠ è½½LLM

ä½¿ç”¨transformersåº“çš„AutoModelForCausalLMæ¥åŠ è½½æ¨¡å‹ï¼Œcache_dirä¸ºä¸‹è½½æ¨¡å‹çš„ç›®å½•ã€‚

```python
cache_dir = "./cache"

nf4_config = BitsAndBytesConfig(
   load_in_4bit=True,
   bnb_4bit_quant_type="nf4",
   bnb_4bit_use_double_quant=True,
   bnb_4bit_compute_dtype=torch.bfloat16
)

# å¾æŒ‡å®šçš„æ¨¡å‹åç¨±æˆ–è·¯å¾‘è¼‰å…¥é è¨“ç·´çš„èªè¨€æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    cache_dir=cache_dir,
    quantization_config=nf4_config,
    low_cpu_mem_usage = True
)

```

## åŠ è½½tokenizer

åŠ è½½LLMçš„tokenizerã€‚åœ¨LLMä¸­ï¼Œtokenizeræ˜¯ä¸€ä¸ªå…³é”®ç»„ä»¶ï¼Œç”¨äºå°†è¾“å…¥çš„æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—åºåˆ—ã€‚

```python
# å‰µå»º tokenizer ä¸¦è¨­å®šçµæŸç¬¦è™Ÿ (eos_token)
logging.getLogger('transformers').setLevel(logging.ERROR)
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    add_eos_token=True,
    cache_dir=cache_dir,
    quantization_config=nf4_config
)
tokenizer.pad_token = tokenizer.eos_token
```

## è®¾ç½®è§£ç å‚æ•°

è®¾ç½®æ¨¡å‹ä½œinferenceæ—¶çš„decoding parametersï¼›

```python
# è¨­å®šæ¨¡å‹æ¨ç†æ™‚éœ€è¦ç”¨åˆ°çš„decoding parameters
max_len = 128
generation_config = GenerationConfig(
    do_sample=True,
    temperature=0.1,
    num_beams=1,
    top_p=0.3,
    no_repeat_ngram_size=3,
    pad_token_id=2,
)
```

## LLM å’Œ tokenizer ä½¿ç”¨ç¤ºä¾‹

ä¸‹é¢çš„ä»£ç ä½¿ç”¨instructionå’Œpoemç»„æˆä¸€ä¸ªprompt:

```python
instruction = 'ä»¥ä¸‹æ˜¯ä¸€é¦–å”è©©çš„ç¬¬ä¸€å¥è©±ï¼Œè«‹ç”¨ä½ çš„çŸ¥è­˜åˆ¤æ–·ä¸¦å®Œæˆæ•´é¦–è©©ã€‚'
poem = 'ç›¸è¦‹æ™‚é›£åˆ¥äº¦é›£ï¼Œæ±é¢¨ç„¡åŠ›ç™¾èŠ±æ®˜ã€‚'

prompt = f"""\
[INST] <<SYS>>
You are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„åŠ©æ‰‹ä¸”æ“…é•·å¯«å”è©©ã€‚
<</SYS>>

{instruction}
{poem}
[/INST]"""
print(prompt)
```

![](../assets/images/Hung-yi_Lee/hw5-7.png)

ä½¿ç”¨tokenizerå¯¹promptåšåˆ†è¯ï¼Œå¾—åˆ°åˆ†è¯åå„ä¸ªtokençš„token_id:

```python
inputs = tokenizer(prompt, return_tensors="pt")
print(inputs)
```

```
{'input_ids': tensor([[    1,   733, 16289, 28793,  2087, 18741,  4060,    13,  1976,   460,
           264, 10865, 13892,   304,  1179,   438,  3653,   320,   602, 16067,
         28723, 28705, 44845, 42171, 51736, 30278, 43308, 51301, 29958, 45695,
         32746, 59631, 28944,    13, 28789,   700, 18741,  4060,    13,    13,
         42564, 28971, 47223, 59631, 28914, 42436, 50175, 28924, 30539, 28963,
         42378, 42546, 43316, 31439, 42292, 29681, 29993, 34965, 28944,    13,
         52324, 29607, 35512, 30798, 32026, 35512, 28924, 55607, 45898, 30421,
         30064, 33504, 28944,    13, 28792, 28748, 16289, 28793,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1]])}
```

è¾“å‡º`input_ids`å’Œ`attention_mask`ç»„æˆçš„dictã€‚

- input_ids: sizeä¸º[batch, åˆ†æ­¤åtokensçš„ä¸ªæ•°]ï¼Œå…¶ä¸­æ¯æ­¤è¡¨ä¸­çš„ä¸ªå…ƒç´ æ˜¯åˆ†è¯åçš„tokenåœ¨è¯è¡¨ä¸­çš„token_idã€‚

```python
inputs_ids = inputs['input_ids']
inputs_ids.size() 
```

```
torch.Size([1, 79])
```

ä¸ºäº†éªŒè¯ä¸€ä¸‹ï¼Œæˆ‘ä»¬å°†token_idè¿˜åŸå›tokenï¼Œè·å¾—token_idå¯¹åº”çš„è¯ã€‚

```python
tokenizer.convert_ids_to_tokens([1,   733, 16289, 28793,  2087, 18741,  4060])
```

```
['<s>', 'â–[', 'INST', ']', 'â–<<', 'SYS', '>>']
```

- attention_mask: sizeä¸input_idsä¸€æ ·ï¼Œå…ƒç´ å€¼ä¸º0/1ï¼Œ1ä»£è¡¨è¿™ä¸ªtokenæœ‰ç”¨ï¼Œ0ä»£è¡¨æ— ç”¨ã€‚

  attention_maskçš„ä½œç”¨æ˜¯ï¼šå½“batchå¤§äº1æ—¶ï¼Œæ¯ä¸€è¡Œå…ƒç´ çš„é•¿åº¦å€¼æ˜¯åˆ†è¯åå­—ç¬¦æ•°é‡æœ€å¤§çš„ï¼Œæ¯”å¦‚ä¸¤è¡Œtextåšåˆ†è¯ï¼Œä¸€è¡Œè¢«åˆ†ä¸º10ä¸ªtokenï¼Œä¸€è¡Œè¢«åˆ†ä¸º20ä¸ªtokenï¼Œé‚£ä¹ˆåˆ†è¯åçš„input_idså’Œattention_maskçš„sizeä¸º[2, 20]ï¼Œattention_mask[0]å°±æ˜¯å‰10ä¸ªå…ƒç´ ä¸º1ï¼Œå10ä¸ªä¸º0ã€‚attention_mask[1]å…¨éƒ¨ä¸º1ã€‚

å°†`inputs_ids`è¾“å…¥LLMç”Ÿæˆæ–‡æœ¬ã€‚

```python
# ä½¿ç”¨æ¨¡å‹é€²è¡Œç”Ÿæˆå›è¦†
generation_output = model.generate(
    input_ids=inputs_ids,
    generation_config=generation_config,
    return_dict_in_generate=True,
    output_scores=True,
    max_new_tokens=max_len,
    )
generation_output.sequences
```

è¾“å‡ºä¸ºä¸€ç»„token_idsã€‚

```python
tensor([[    1,   733, 16289, 28793,  2087, 18741,  4060,    13,  1976,   460,
           264, 10865, 13892,   304,  1179,   438,  3653,   320,   602, 16067,
         28723, 28705, 44845, 42171, 51736, 30278, 43308, 51301, 29958, 45695,
         32746, 59631, 28944,    13, 28789,   700, 18741,  4060,    13,    13,
         42564, 28971, 47223, 59631, 28914, 42436, 50175, 28924, 30539, 28963,
         42378, 42546, 43316, 31439, 42292, 29681, 29993, 34965, 28944,    13,
         52324, 29607, 35512, 30798, 32026, 35512, 28924, 55607, 45898, 30421,
         30064, 33504, 28944,    13, 28792, 28748, 16289, 28793,     2, 28705,
         29367, 30606, 29607, 35512, 29062, 32026, 35512, 30798, 28924, 55607,
         32329, 33089, 30421, 30064, 33485, 33504, 33504, 28944,     2]])
```

æŠŠè¾“å‡ºçš„token_idsäº¤ç»™tokenizeråšdecodeè§£ç ï¼Œå¯ä»¥çœ‹åˆ°è¾“å‡ºä¸€æ®µæ–‡å­—ã€‚

```python
tokenizer.decode(generation_output.sequences[0])
```

```
<s> [INST] <<SYS>>\nYou are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„åŠ©æ‰‹ä¸”æ“…é•·å¯«å”è©©ã€‚\n<</SYS>>\n\nä»¥ä¸‹æ˜¯ä¸€é¦–å”è©©çš„ç¬¬ä¸€å¥è©±ï¼Œè«‹ç”¨ä½ çš„çŸ¥è­˜åˆ¤æ–·ä¸¦å®Œæˆæ•´é¦–è©©ã€‚\nç›¸è¦‹æ™‚é›£åˆ¥äº¦é›£ï¼Œæ±é¢¨ç„¡åŠ›ç™¾èŠ±æ®˜ã€‚\n[/INST]</s> ç›¸æœ›æ™‚é›£åˆ†äº¦é›£åˆ¥ï¼Œæ±é¢¨å¹æ‹‚ç™¾èŠ±æ¬²æ®˜æ®˜ã€‚</s>
```

## generate_training_dataå‡½æ•°

`generate_training_data()`è¾“å…¥`data_point`(instruction+input+output)ï¼Œè¾“å‡ºæ¨¡å‹å¯ä»¥è¯»å–çš„tokenã€‚

1. æŒ‰ç…§å›ºå®šçš„æ ¼å¼å°†`data_point`è½¬åŒ–æˆprompt:

   ```python
   # construct full input prompt
       prompt = f"""\
   [INST] <<SYS>>
   You are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„åŠ©æ‰‹ä¸”æ“…é•·å¯«å”è©©ã€‚
   <</SYS>>
   
   {data_point["instruction"]}
   {data_point["input"]}
   [/INST]"""
   ```

2. ä½¿ç”¨`tokenizer`è§£æpromptï¼Œè¿”å›å«"input_ids"ã€"labels"å’Œ"attention_mask"çš„dictã€‚

   ```python
   # transform input prompt into tokens
       full_tokens = tokenizer(
           prompt + " " + data_point["output"] + "</s>",
           truncation=True,
           max_length=CUTOFF_LEN + 1,
           padding="max_length",
       )["input_ids"][:-1]
       return {
           "input_ids": full_tokens,
           "labels": [-100] * len_user_prompt_tokens
           + full_tokens[len_user_prompt_tokens:],
           "attention_mask": [1] * (len(full_tokens)),
       }
   
   ```

## evaluateå‡½æ•°

`evaluate()`å‡½æ•°è¾“å…¥instructionã€generation_configã€max_lenã€inputï¼Œè¾“å‡ºæ¨¡å‹çš„å“åº”ã€‚

generation_configé¢„å…ˆè®¾ç½®å¦‚ä¸‹ï¼š

```python
# è¨­å®šæ¨¡å‹æ¨ç†æ™‚éœ€è¦ç”¨åˆ°çš„decoding parameters
max_len = 128
generation_config = GenerationConfig(
    do_sample=True,
    temperature=0.1,
    num_beams=1,
    top_p=0.3,
    no_repeat_ngram_size=3,
    pad_token_id=2,
)
```

`GenerationConfig` æ˜¯ä¸€ä¸ªç”¨äºé…ç½®æ–‡æœ¬ç”Ÿæˆæ¨¡å‹çš„ç±»ã€‚å®ƒå…è®¸ç”¨æˆ·è®¾ç½®ç”Ÿæˆè¿‡ç¨‹ä¸­ä½¿ç”¨çš„å„ç§å‚æ•°ï¼Œå¦‚æœ€å¤§ç”Ÿæˆé•¿åº¦ã€æ¸©åº¦ã€é¡¶çº§é‡‡æ ·ç­‰ã€‚

## Set Hyperarameters for Fine-tuning

```python
num_train_data = 1040 # è¨­å®šç”¨ä¾†è¨“ç·´çš„è³‡æ–™æ•¸é‡ï¼Œå¯è¨­ç½®çš„æœ€å¤§å€¼ç‚º5000ã€‚åœ¨å¤§éƒ¨åˆ†æƒ…æ³ä¸‹æœƒå¸Œæœ›è¨“ç·´è³‡æ–™ç›¡é‡è¶Šå¤šè¶Šå¥½ï¼Œé€™æœƒè®“æ¨¡å‹çœ‹éæ›´å¤šæ¨£åŒ–çš„è©©å¥ï¼Œé€²è€Œæå‡ç”Ÿæˆå“è³ªï¼Œä½†æ˜¯ä¹Ÿæœƒå¢åŠ è¨“ç·´çš„æ™‚é–“
                      # ä½¿ç”¨é è¨­åƒæ•¸(1040): fine-tuningå¤§ç´„éœ€è¦25åˆ†é˜ï¼Œå®Œæ•´è·‘å®Œæ‰€æœ‰cellå¤§ç´„éœ€è¦50åˆ†é˜
                      # ä½¿ç”¨æœ€å¤§å€¼(5000): fine-tuningå¤§ç´„éœ€è¦100åˆ†é˜ï¼Œå®Œæ•´è·‘å®Œæ‰€æœ‰cellå¤§ç´„éœ€è¦120åˆ†é˜
```

ä¸å»ºè®®ä¿®æ”¹çš„å‚æ•°ï¼š

```python
""" It is recommmended NOT to change codes in this cell """

cache_dir = "./cache"  # è¨­å®šå¿«å–ç›®éŒ„è·¯å¾‘
from_ckpt = False  # æ˜¯å¦å¾checkpointè¼‰å…¥æ¨¡å‹çš„æ¬Šé‡ï¼Œé è¨­ç‚ºå¦
ckpt_name = None  # å¾ç‰¹å®šcheckpointè¼‰å…¥æ¬Šé‡æ™‚ä½¿ç”¨çš„æª”æ¡ˆåç¨±ï¼Œé è¨­ç‚ºç„¡
dataset_dir = "./GenAI-Hw5/Tang_training_data.json"  # è¨­å®šè³‡æ–™é›†çš„ç›®éŒ„æˆ–æª”æ¡ˆè·¯å¾‘
logging_steps = 20  # å®šç¾©è¨“ç·´éç¨‹ä¸­æ¯éš”å¤šå°‘æ­¥é©Ÿè¼¸å‡ºä¸€æ¬¡è¨“ç·´èªŒ
save_steps = 65  # å®šç¾©è¨“ç·´éç¨‹ä¸­æ¯éš”å¤šå°‘æ­¥é©Ÿä¿å­˜ä¸€æ¬¡æ¨¡å‹
save_total_limit = 3  # æ§åˆ¶æœ€å¤šä¿ç•™å¹¾å€‹æ¨¡å‹checkpoint
report_to = None  # è¨­å®šä¸Šå ±å¯¦é©—æŒ‡æ¨™çš„ç›®æ¨™ï¼Œé è¨­ç‚ºç„¡
MICRO_BATCH_SIZE = 4  # å®šç¾©å¾®æ‰¹æ¬¡çš„å¤§å°
BATCH_SIZE = 16  # å®šç¾©ä¸€å€‹æ‰¹æ¬¡çš„å¤§å°
GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE  # è¨ˆç®—æ¯å€‹å¾®æ‰¹æ¬¡ç´¯ç©çš„æ¢¯åº¦æ­¥æ•¸
CUTOFF_LEN = 256  # è¨­å®šæ–‡æœ¬æˆªæ–·çš„æœ€å¤§é•·åº¦
LORA_R = 8  # è¨­å®šLORAï¼ˆLayer-wise Random Attentionï¼‰çš„Rå€¼
LORA_ALPHA = 16  # è¨­å®šLORAçš„Alphaå€¼
LORA_DROPOUT = 0.05  # è¨­å®šLORAçš„Dropoutç‡
VAL_SET_SIZE = 0  # è¨­å®šé©—è­‰é›†çš„å¤§å°ï¼Œé è¨­ç‚ºç„¡
TARGET_MODULES = ["q_proj", "up_proj", "o_proj", "k_proj", "down_proj", "gate_proj", "v_proj"] # è¨­å®šç›®æ¨™æ¨¡çµ„ï¼Œé€™äº›æ¨¡çµ„çš„æ¬Šé‡å°‡è¢«ä¿å­˜ç‚ºcheckpoint
device_map = "auto"  # è¨­å®šè¨­å‚™æ˜ å°„ï¼Œé è¨­ç‚º"auto"
world_size = int(os.environ.get("WORLD_SIZE", 1))  # ç²å–ç’°å¢ƒè®Šæ•¸"WORLD_SIZE"çš„å€¼ï¼Œè‹¥æœªè¨­å®šå‰‡é è¨­ç‚º1
ddp = world_size != 1  # æ ¹æ“šworld_sizeåˆ¤æ–·æ˜¯å¦ä½¿ç”¨åˆ†æ•£å¼æ•¸æ“šè™•ç†(DDP)ï¼Œè‹¥world_sizeç‚º1å‰‡ä¸ä½¿ç”¨DDP
if ddp:
    device_map = {"": int(os.environ.get("LOCAL_RANK") or 0)}
    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size
```

## Start Fine-tuning

å°†æ¨¡å‹å‡†å¤‡å¥½ï¼Œå¹¶ä½¿ç”¨ INT8è®­ç»ƒï¼š

```python
# å°‡æ¨¡å‹æº–å‚™å¥½ä»¥ä½¿ç”¨ INT8 è¨“ç·´
model = prepare_model_for_int8_training(model)
```

`prepare_model_for_int8_training`æ˜¯Hugging Faceçš„PEFT(Parameter-Efficient Fine-Tuning)åº“ä¸­çš„æ–¹æ³•ï¼Œç”¨äºå‡†å¤‡æ¨¡å‹è¿›è¡ŒINT8è®­ç»ƒã€‚INT8è®­ç»ƒæ˜¯å°†æ¨¡å‹çš„æƒé‡è½¬æ¢ä¸º8ä½æ•´æ•°ï¼ˆINT8ï¼‰ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨å’ŒåŠ é€Ÿè®¡ç®—ï¼ŒåŒæ—¶å°½é‡ä¿æŒæ¨¡å‹çš„æ€§èƒ½ã€‚

```python
# ä½¿ç”¨ LoraConfig é…ç½® LORA æ¨¡å‹
config = LoraConfig(
    r=LORA_R,
    lora_alpha=LORA_ALPHA,
    target_modules=TARGET_MODULES,
    lora_dropout=LORA_DROPOUT,
    bias="none",
    task_type="CAUSAL_LM",
)
```

ä»åŸå§‹æ•°æ®é›†ä¸­åŠ è½½è®­ç»ƒæ•°æ®ï¼Œå°†num_train_data = 1040ç¬”æ•°æ®å†™å…¥`tmp_dataset.json`æ–‡ä»¶ã€‚ç„¶åä½¿ç”¨

```python
# è¼‰å…¥ä¸¦è™•ç†è¨“ç·´æ•¸æ“š
with open(dataset_dir, "r", encoding = "utf-8") as f:
    data_json = json.load(f)
with open("tmp_dataset.json", "w", encoding = "utf-8") as f:
    json.dump(data_json[:num_train_data], f, indent = 2, ensure_ascii = False)
data = load_dataset('json', data_files="tmp_dataset.json", download_mode="force_redownload")
```

æ•°æ®è¢«åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚`VAL_SET_SIZE`é¢„è®¾ä¸º0ï¼Œè¡¨ç¤ºæ²¡æœ‰éªŒè¯é›†æ•°æ®ã€‚

```python
# å°‡è¨“ç·´æ•¸æ“šåˆ†ç‚ºè¨“ç·´é›†å’Œé©—è­‰é›†ï¼ˆè‹¥ VAL_SET_SIZE å¤§æ–¼ 0ï¼‰
if VAL_SET_SIZE > 0:
    train_val = data["train"].train_test_split(
        test_size=VAL_SET_SIZE, shuffle=True, seed=42
    )
    train_data = train_val["train"].shuffle().map(generate_training_data)
    val_data = train_val["test"].shuffle().map(generate_training_data)
else:
    train_data = data['train'].shuffle().map(generate_training_data)
    val_data = None
```

ä½¿ç”¨Transformerçš„Trainerè®­ç»ƒæ¨¡å‹

```python
# ä½¿ç”¨ Transformers Trainer é€²è¡Œæ¨¡å‹è¨“ç·´
trainer = transformers.Trainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=val_data,
    args=transformers.TrainingArguments(
        per_device_train_batch_size=MICRO_BATCH_SIZE,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        warmup_steps=50,
        num_train_epochs=num_epoch,
        learning_rate=LEARNING_RATE,
        fp16=True,  # ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´
        logging_steps=logging_steps,
        save_strategy="steps",
        save_steps=save_steps,
        output_dir=ckpt_dir,
        save_total_limit=save_total_limit,
        ddp_find_unused_parameters=False if ddp else None,  # æ˜¯å¦ä½¿ç”¨ DDPï¼Œæ§åˆ¶æ¢¯åº¦æ›´æ–°ç­–ç•¥
        report_to=report_to,
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

......

# é–‹å§‹æ¨¡å‹è¨“ç·´
trainer.train()
```

å¯ä»¥çœ‹åˆ°ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒçš„ç»“æœï¼Œlossé€æ¸é™ä½ã€‚

```python
{'loss': 3.3137, 'grad_norm': 2.415785551071167, 'learning_rate': 0.00011399999999999999, 'epoch': 0.31}
{'loss': 2.0632, 'grad_norm': 2.1237754821777344, 'learning_rate': 0.000234, 'epoch': 0.62}
{'loss': 1.9832, 'grad_norm': 1.4616379737854004, 'learning_rate': 0.00011999999999999999, 'epoch': 0.92}
```

## Testing

æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹çœ‹çœ‹æ•ˆæœã€‚

### åŠ è½½checkpoints

```python
# find all available checkpoints
ckpts = []
for ckpt in os.listdir(ckpt_dir):
    if (ckpt.startswith("checkpoint-")):
        ckpts.append(ckpt)

# list all the checkpoints
ckpts = sorted(ckpts, key = lambda ckpt: int(ckpt.split("-")[-1]))
print("all available checkpoints:")
print(" id: checkpoint name")
for (i, ckpt) in enumerate(ckpts):
    print(f"{i:>3}: {ckpt}")

```

