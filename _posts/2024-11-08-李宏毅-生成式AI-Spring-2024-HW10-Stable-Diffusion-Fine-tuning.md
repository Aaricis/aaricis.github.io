---
title: ã€æå®æ¯…-ç”Ÿæˆå¼AIã€‘Spring 2024, HW10ï¼šStable Diffusion Fine-tuning
date: 2024-11-08 16:40:00 +/-8
categories: [LLM, Spring 24 GenAI]
tags: [Hung-yi Lee]     # TAG names should always be lowercase
---

## Overview

### Text-to-Image Model

Text-to-Image Modelå¯ä»¥ç”Ÿæˆä¸æ–‡æœ¬æè¿°ç›¸åŒ¹é…çš„å›¾åƒã€‚ChatGPT-4Oå…·å¤‡Text-to-Imageèƒ½åŠ›ï¼Œæˆ‘ä»¬ç”¨ChatGPT-4Oè¯•ä¸€ä¸‹ï¼Œè¾“å…¥æ–‡å­—æè¿°ï¼Œæ¨¡å‹æ ¹æ®æ–‡å­—æè¿°è¾“å‡ºå›¾ç‰‡ğŸ‘‡

![](../assets/images/Hung-yi_Lee/hw10-1.png)

![](../assets/images/Hung-yi_Lee/hw10-2.png)

### Personalization

åŸºç¡€æ¨¡å‹ä¸èƒ½æ»¡è¶³ä¸ªæ€§åŒ–çš„è¦æ±‚ï¼Œä¾‹å¦‚ChatGPT-4Oæ¯æ¬¡è¾“å‡ºçš„äººè„¸éƒ½ä¸ä¸€æ ·ã€‚æƒ³è¦æ¨¡å‹æ¯æ¬¡è¾“å‡ºç‰¹å®šçš„äººç‰©å½¢è±¡ï¼Œä¾‹å¦‚æ¯æ¬¡éƒ½ç”ŸæˆBrad Pittçš„è„¸ï¼Œéœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

![](../assets/images/Hung-yi_Lee/hw10-3.png)

## Achieve Personalization Using LoRA

###  LoRA

ä½¿ç”¨LoRAï¼ˆLow-Rank Adaptationï¼‰å¯¹åŸºç¡€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚LoRAæ˜¯å¾®è½¯åœ¨2021å¹´æ¨å‡ºçš„å¾®è°ƒæ–¹æ³•ï¼Œå®ƒå†»ç»“é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡ï¼Œå°†å¯è®­ç»ƒçš„ç§©åˆ†è§£çŸ©é˜µæ³¨å…¥åˆ°Transformeræ¶æ„çš„æ¯ä¸€å±‚ï¼Œä»è€Œå¤§å¤§å‡å°‘ä¸‹æ¸¸ä»»åŠ¡çš„å¯è®­ç»ƒå‚æ•°é‡ã€‚å¤šæ•°æƒ…å†µä¸‹ï¼ŒLoRAçš„æ€§èƒ½å¯ä¸å®Œå…¨å¾®è°ƒçš„æ¨¡å‹ç›¸åª²ç¾ï¼Œå¹¶ä¸”ä¸ä¼šå¼•å…¥é¢å¤–çš„æ¨ç†æ—¶å»¶ã€‚

ç¥ç»ç½‘ç»œè¿›è¡ŒçŸ©é˜µä¹˜æ³•æ—¶ï¼Œå®ƒçš„æƒé‡çŸ©é˜µæ˜¯full-rankçš„ã€‚åœ¨é€‚é…ç‰¹å®šä»»åŠ¡æ—¶ï¼Œé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹å…·æœ‰è¾ƒä½çš„â€å†…åœ¨ç»´åº¦ï¼ˆintrinsic dimensionï¼‰â€œï¼Œåœ¨éšæœºæŠ•å°„çš„æ›´å°å­ç©ºé—´ä¸Šä»»ç„¶å¯ä»¥æœ‰æ•ˆçš„å­¦ä¹ ã€‚â€å†…åœ¨ç»´åº¦ï¼ˆintrinsic dimensionï¼‰â€œæŒ‡çŸ©é˜µä¸­å­˜åœ¨æœ‰æ•ˆä¿¡æ¯çš„ç»´åº¦ï¼Œå°äºç­‰äºçŸ©é˜µçš„å®é™…ç»´åº¦ã€‚å—æ­¤å¯å‘ï¼ŒLoRAå‡è®¾åœ¨è®¾é…ç‰¹å®šä»»åŠ¡æ—¶ï¼ŒçŸ©é˜µæƒé‡æ›´æ–°å…·æœ‰è¾ƒä½çš„å†…åœ¨ç»´åº¦ï¼Œå³å¯¹æ¨¡å‹å¾®è°ƒå°±æ˜¯è°ƒæ•´è¿™äº›ä½ç§©çš„å†…åœ¨ç»´åº¦ã€‚

![](../assets/images/Hung-yi_Lee/hw10-4.png)

LoRAåœ¨åŸæœ¬çš„çŸ©é˜µ$W\in R^{d\times k}$æ—è¾¹æ’å…¥ä¸€ä¸ªå¹¶è¡Œçš„æƒé‡çŸ©é˜µ$\Delta W \in R^{d \times k}$ã€‚å› ä¸ºæ¨¡å‹çš„ä½ç§©æ€§ï¼Œ$\Delta W$å¯è¢«æ‹†åˆ†ä¸ºçŸ©é˜µ$B\in R^{d \times r}$å’Œ$A\in R^{r\times k}$ï¼Œå…¶ä¸­$r\ll min(d, k)$ï¼Œä»è€Œå®ç°äº†æå°çš„å‚æ•°æ•°é‡è®­ç»ƒLLMã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œ$W$è¢«å†»ç»“ï¼Œä¸ä¼šæ¥å—æ¢¯åº¦æ›´æ–°ï¼Œè€Œ$A$å’Œ$B$åŒ…å«å¯è®­ç»ƒçš„å‚æ•°ä¼šè¢«æ›´æ–°ã€‚$W$å’Œ$\Delta W$æ¥å—ç›¸åŒçš„è¾“å…¥$x$ï¼Œè®­ç»ƒå®Œæˆåå„è‡ªçš„è¾“å‡ºå‘é‡æŒ‰ä½ç½®ç›¸åŠ ï¼Œå› æ­¤ä¸ä¼šäº§ç”Ÿé¢å¤–çš„æ¨ç†æ—¶é—´ï¼Œå¦‚ä¸‹å¼æ‰€ç¤ºï¼š
$$
h = W_0 x + \Delta W x = W_0 x + BAx
$$
åœ¨è®­ç»ƒå¼€å§‹çš„æ—¶å€™ï¼Œå¯¹çŸ©é˜µ$A$è¿›è¡Œéšæœºé«˜æ–¯åˆå§‹åŒ–ï¼ŒçŸ©é˜µ$B$ä½¿ç”¨é›¶çŸ©é˜µåˆå§‹åŒ–ã€‚å› ä¸º$r$é€šå¸¸æ˜¯ä¸€ä¸ªéå¸¸å°çš„å€¼ï¼ˆå®éªŒè¯æ˜1ã€2ã€4ã€8çš„æ•ˆæœå°±éå¸¸å¥½ï¼‰ï¼Œæ‰€ä»¥LoRAåœ¨è®­ç»ƒæ—¶å¼•å…¥çš„å‚æ•°é‡æ˜¯éå¸¸å°çš„ï¼Œå› æ­¤å®ƒçš„è®­ç»ƒéå¸¸é«˜æ•ˆï¼Œä¸ä¼šå¸¦æ¥æ˜¾è‘—çš„æ˜¾å­˜å¢åŠ ã€‚

### Fine-tuning with LoRA

ä½¿ç”¨LoRAå¾®è°ƒåŸºç¡€æ¨¡å‹ï¼Œè·å¾—é¢å¤–çš„personalizationèƒ½åŠ›ã€‚

![](../assets/images/Hung-yi_Lee/hw10-5.PNG)

## Goal of This Homework

ä½¿ç”¨åŒä¸€ä¸ªäººçš„é¢éƒ¨å›¾ç‰‡å¾®è°ƒStable Diffusionæ¨¡å‹ï¼Œä½¿å®ƒç”Ÿæˆé¢å­”ä¸€è‡´çš„å›¾ç‰‡ã€‚

![](../assets/images/Hung-yi_Lee/hw10-6.PNG)

åŸå§‹çš„Stable Diffusionæ¨¡å‹æ¯æ¬¡äº§ç”Ÿçš„å›¾ç‰‡äººè„¸éƒ½ä¸ä¸€æ ·ã€‚æˆ‘ä»¬ä½¿ç”¨Brad Pittçš„å›¾ç‰‡å’Œå¯¹åº”çš„æ–‡æœ¬æè¿°å¾®è°ƒåŸå§‹æ¨¡å‹ï¼Œä½¿å®ƒäº§ç”Ÿçš„å›¾ç‰‡éƒ½æ˜¯Brad Pittçš„è„¸ã€‚

ç»™å®šå……åˆ†çš„è®­ç»ƒæ—¶é—´ï¼ŒStable Diffusionç”Ÿæˆçš„å›¾ç‰‡å°†ä¼šæ‹¥æœ‰åŒä¸€ä¸ªäººçš„è„¸ã€‚

![](../assets/images/Hung-yi_Lee/hw10-7.PNG)

## Evaluation

å°†ç»™å®šçš„promptè¾“å…¥å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆå›¾ç‰‡ï¼Œæœ€ç»ˆçš„å¾—åˆ†å°†ä¼šä»ä¸‰ä¸ªæ–¹é¢è¿›è¡Œè¯„ä¼°ï¼š

- å›¾ç‰‡æ˜¯å¦ä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼ï¼Ÿ
- å›¾ç‰‡å’Œæ–‡å­—æ˜¯å¦åŒ¹é…ï¼Ÿ
- å›¾ç‰‡æ˜¯å¦åŒ…å«äººè„¸ï¼Ÿ

## Task Introduction

![](../assets/images/Hung-yi_Lee/hw10-8.PNG)

## Step 1. Fine-tune Stable Diffusion

Stable Diffusionæ˜¯ä¸€ä¸ªtext-to-imageæ½œåœ¨æ‰©æ•£([Latent Diffusion](https://arxiv.org/abs/2112.10752))æ¨¡å‹ï¼Œç”±[CompVis](https://github.com/CompVis)ã€[Stability AI](https://stability.ai/)å’Œ[LAION](https://laion.ai/)çš„ç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆå…±åŒåˆ›å»ºã€‚å®ƒä½¿ç”¨512x512å¤§å°çš„å›¾ç‰‡è®­ç»ƒï¼Œæ•°æ®é›†æ˜¯[LAION-5B](https://laion.ai/blog/laion-5b/)çš„å­é›†ã€‚LAION-5Bæ˜¯ç°å­˜æœ€å¤§å¯è‡ªç”±è®¿é—®çš„å¤šæ¨¡æ€æ•°æ®åº“ã€‚Latent Diffusionæœ‰ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ï¼š

1. An autoencoder(VAE);
2. A U-Net;
3. A text-encoder, e.g. CLIP's Text Encoder;

åœ¨Stable Diffusionæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œå¾®è°ƒï¼Œä»¥å®ç°ä¸ªæ€§åŒ–çš„éœ€æ±‚ã€‚ä½¿ç”¨Hugging Face `stablediffusionapi/cyberrealistic-41`æ¨¡å‹ä½œä¸ºbase modelï¼›è®­ç»ƒæ•°æ®ä¸º100å¼ Brad Pittçš„ç…§ç‰‡å’Œå¯¹åº”çš„æ–‡æœ¬æè¿°ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç…§ç‰‡å’Œæ–‡æœ¬æè¿°æˆå¯¹å‡ºç°ï¼Œå…·æœ‰ç›¸åŒçš„æ–‡ä»¶åã€‚

![](../assets/images/Hung-yi_Lee/hw10-9.png)



### å®‰è£…å¿…è¦çš„åº“ï¼š

```python
# Install the required packages
os.chdir(root_dir)
!pip -q install timm==1.0.7
!pip -q install fairscale==0.4.13
!pip -q install transformers==4.41.2
!pip -q install requests==2.32.3
!pip -q install accelerate==0.31.0
!pip -q install diffusers==0.29.1
!pip -q install einop==0.0.1
!pip -q install safetensors==0.4.3
!pip -q install voluptuous==0.15.1
!pip -q install jax==0.4.33
!pip -q install peft==0.11.1
!pip -q install deepface==0.0.92
!pip -q install tensorflow==2.17.0
!pip -q install keras==3.2.0
```

### å¯¼å…¥å¿…è¦çš„åŒ…ï¼š

```python
#@markdown ##  Import necessary packages
#@markdown It is recommmended NOT to change codes in this cell.
import argparse
import logging
import math
import os
import random
import glob
import shutil
from pathlib import Path
import numpy as np
import torch
import torch.nn.functional as F
import torch.utils.checkpoint
import transformers

# Python Imaging Libraryï¼ˆPILï¼‰å›¾åƒå¤„ç†
from PIL import Image

# å›¾åƒå¤„ç†
from torchvision import transforms
from torchvision.utils import save_image

# æ˜¾ç¤ºè¿›åº¦æ¡
from tqdm.auto import tqdm

# Parameter-Efficient Fine-tuning(PEFT)åº“
from peft import LoraConfig
from peft.utils import get_peft_model_state_dict

# Hugging Face transformers
from transformers import AutoProcessor, AutoModel, CLIPTextModel, CLIPTokenizer

# Hugging Face Diffusion Modelåº“
import diffusers
from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel
from diffusers.optimization import get_scheduler
from diffusers.utils import convert_state_dict_to_diffusers
from diffusers.training_utils import compute_snr
from diffusers.utils.torch_utils import is_compiled_module

# é¢éƒ¨æ£€æµ‹
from deepface import DeepFace

# OpenCV
import cv2
```

### å‚æ•°è®¾ç½®

```python
output_folder = os.path.join(project_dir, "logs") # å­˜æ”¾model checkpointsè·Ÿvalidationçµæœçš„è³‡æ–™å¤¾
seed = 1126 # random seed
train_batch_size = 2 # training batch size
resolution = 512 # Image size
weight_dtype = torch.bfloat16 # æ¨¡å‹æƒé‡çš„æ•°æ®ç±»å‹æ˜¯bfloat16
snr_gamma = 5 # Signal-to-Noise Ratio(SNR)ä¿¡å™ªæ¯”ç¼©æ”¾å› å­

## Important parameters for fine-tuning Stable Diffusion
pretrained_model_name_or_path = "stablediffusionapi/cyberrealistic-41"
lora_rank = 32 # r(rank)
lora_alpha = 16 # Loraçš„alphaå‚æ•°

# å­¦ä¹ ç‡
learning_rate = 1e-4 #@param {type:"number"}
unet_learning_rate = learning_rate
text_encoder_learning_rate = learning_rate
lr_scheduler_name = "cosine_with_restarts" # ä½¿ç”¨Cosine Annealing with Restartså­¦ä¹ ç‡è°ƒåº¦æ–¹æ³•
lr_warmup_steps = 100 # åˆå§‹é¢„çƒ­æ­¥æ•°

# æœ€å¤§è®­ç»ƒæ­¥æ•°
max_train_steps = 200 

# éªŒè¯é›†æ•°æ®
validation_prompt = "validation_prompt.txt"
validation_prompt_path = os.path.join(prompts_folder, validation_prompt)
validation_prompt_num = 3 #@param {type:"slider", min:1, max:5, step:1}
validation_step_ratio = 1 #@param {type:"slider", min:0, max:1, step:0.1}
with open(validation_prompt_path, "r") as f:
    validation_prompt = [line.strip() for line in f.readlines()]
```

### è®¾ç½®LoRA Config

> åŸç‰ˆä»£ç å¯¼å…¥äº†`peft`å¹¶è®¾ç½®äº†`lora_rank`å’Œ`lora_alpha`å‚æ•°ï¼Œä½†æ˜¯æ²¡æœ‰çœŸæ­£ä½¿ç”¨LoRAä½œå¾®è°ƒã€‚å‡ºäºå­¦ä¹ çš„ç›®çš„ï¼Œç¬”è€…æ”¹å†™äº†æºä»£ç ï¼Œæä¾›ä¸€ç‰ˆä½¿ç”¨LoRAå¾®è°ƒçš„ä»£ç ï¼Œä¾›å¤§å®¶å­¦ä¹ äº¤æµã€‚

Stable Diffusionæ¨¡å‹åŒ…å«ä¸‰ä¸ªç»„ä»¶ï¼šCLIPã€U-netã€VAEã€‚å‚æ•°é‡åˆ†å¸ƒå’Œå æ¯”ä¸ºï¼š
[æ¥æº](https://forums.fast.ai/t/stable-diffusion-parameter-budget-allocation/101515)

| ç»„ä»¶      | å‚æ•°é‡ | æ–‡ä»¶å¤§å°  |  å æ¯” |
| ----------- | ----------- |----------- |----------- |
| CLIP      | 123,060,480 |  492 MB  |  12%  |
|  VAE  | 83,653,863 |  335 MB  |  8%  |
| U-net      | 859,520,964 |  3.44 GB  |  80%  |
| Total      | 1,066,235,307 | 4.27 GB |  100%  |

U-netæ˜¯æœ€æ ¸å¿ƒçš„ç»„ä»¶ï¼ŒCLIPç›¸å¯¹ä¹Ÿæ¯”è¾ƒé‡è¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©U-netå’ŒCLIPçš„Attentionæ¨¡å—è¿›è¡Œå¾®è°ƒã€‚

> LoRAçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä½ç§©åˆ†è§£é™ä½å¤§è§„æ¨¡æ¨¡å‹çš„å‚æ•°æ›´æ–°éœ€æ±‚ï¼Œä»è€Œå‡å°‘è®­ç»ƒæˆæœ¬ã€‚å…¶æ•°å­¦å½¢å¼ä¸ºï¼š
> $$
> W' = W + \Delta W
> $$
> å…¶ä¸­ï¼š
>
> - $W$ï¼šåŸå§‹æƒé‡çŸ©é˜µï¼ˆå†»ç»“ï¼Œä¸æ›´æ–°ï¼‰ï¼›
> - $\Delta W$ï¼šé€šè¿‡ä½ç§©åˆ†è§£è·å¾—çš„çŸ©é˜µï¼Œ$\Delta W = AB$
>   - $A$ï¼šä½ç§©çŸ©é˜µï¼Œå½¢çŠ¶ä¸º$(d, r)$ï¼›
>   - $B$ï¼šä½ç§©çŸ©é˜µï¼Œå½¢çŠ¶ä¸º$(r,d)$ï¼›
>   - $r$ï¼šç§©ï¼Œè¿œå°äº$d$ã€‚
>
> åœ¨LoRAä¸­ï¼Œç›´æ¥ä½¿ç”¨ä½ç§©çŸ©é˜µ$AB$æ›´æ–°$\Delta W $æœ‰æ—¶ä¼šå¯¼è‡´ä»¥ä¸‹é—®é¢˜ï¼š
>
> 1. **ä¸ç¨³å®šçš„è®­ç»ƒï¼š**å¦‚æœ$AB$çš„å€¼èŒƒå›´è¿‡å¤§ï¼Œä¸åŸå§‹æƒé‡$W$ç›¸æ¯”å¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šï¼Œæ¨¡å‹éš¾ä»¥æ”¶æ•›ï¼›
> 2. **æ›´æ–°å¹…åº¦è¿‡å¤§ï¼š**å¦‚æœæ²¡æœ‰çº¦æŸï¼Œ$AB$çš„å½±å“å¯èƒ½è¿‡å¼ºï¼Œå¯¼è‡´æ¨¡å‹åç¦»é¢„è®­ç»ƒæƒé‡ï¼›
> 3. **æ— æ³•çµæ´»è°ƒèŠ‚ï¼š**ä¸åŒä»»åŠ¡å¯¹å‚æ•°è°ƒæ•´çš„çµæ•æ€§ä¸åŒï¼Œæœ‰æ—¶éœ€è¦æ›´å¤§æˆ–æ›´å°çš„æƒé‡è´¡çŒ®ã€‚
>
> ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒLoRAå¼•å…¥äº†ä¸€ä¸ªç¼©æ”¾å› å­$\alpha$ï¼Œæ§åˆ¶$\Delta W$çš„å½±å“ï¼š
> $$
> W' = W + \frac \alpha rAB
> $$
>
> - $\alpha$ ï¼šç¼©æ”¾å› å­ï¼Œå³`lora_alpha`ï¼Œç”¨äºè°ƒèŠ‚$AB$å¯¹æ€»æƒé‡æ›´æ–°çš„è´¡çŒ®ï¼›
> - $r$ï¼šä½ç§©çŸ©é˜µçš„ç§©ï¼Œç”¨äºè§„èŒƒåŒ–ã€‚
>
> é€šè¿‡ç¼©æ”¾å› å­$\alpha$ï¼Œå¯ä»¥æœ‰æ•ˆå¹³è¡¡åŸå§‹æƒé‡$W$å’ŒLoRAçŸ©é˜µ$AB$çš„è´¡çŒ®ã€‚

ä½¿ç”¨Hugging Faceçš„[PEFT(Parameter-Efficient Fine-Tuning)](https://huggingface.co/docs/peft/v0.13.0/en/index)å®ç°LoRAå¾®è°ƒğŸ‘‡ï¼Œé¦–å…ˆå®šä¹‰`LoraConfig`:

```python
# Stable Diffusion LoRAè®¾ç½®
lora_config = LoraConfig(
    r=lora_rank, # ä½ç§©çŸ©é˜µçš„ç§©
    lora_alpha=lora_alpha, # ç¼©æ”¾å› å­
    target_modules=[
        "q_proj", "v_proj", "k_proj", "out_proj",  # æŒ‡å®šText encoder(CLIP)çš„LoRAåº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŠ•å½±çŸ©é˜µï¼‰
        "to_k", "to_q", "to_v", "to_out.0"  # æŒ‡å®šUNetçš„LoRAåº”ç”¨å¯¹è±¡ï¼ˆç”¨äºè°ƒæ•´UNetä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰
    ], # åº”ç”¨LoRAçš„æ¨¡å—åç§°
    lora_dropout=0 # LoRAå±‚çš„dropoutæ¦‚ç‡
)
```

target_modulesæŒ‡å®šæ¨¡å‹ç»“æ„ä¸­åº”ç”¨LoRAæœºåˆ¶çš„æ¨¡å—åç§°ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå°†æ ¹æ®æ¨¡å‹ç»“æ„é€‰æ‹©æ¨¡å—ã€‚

### åº”ç”¨LoRA

ä¸ºäº†å°†LoRAåº”ç”¨åˆ°Stable Diffusionçš„Attentionæ¨¡å—ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±æ­å»ºStable Diffusionæ¨¡å‹æ¡†æ¶ã€‚

ä½¿ç”¨Hugging Faceçš„[diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=QQXXMLKkCbUJ)åº“å®ç°å¾®è°ƒStable Diffusionï¼Œdiffusersçš„æ ¸å¿ƒAPIå¯åˆ†ä¸ºä¸‰ä¸ªç»„æˆéƒ¨åˆ†ï¼š

1. **Pipelineï¼ˆç®¡é“ï¼‰**ï¼špipelineæ˜¯diffusersåº“ä¸­ç”¨äºæ„å»ºå’Œè¿è¡Œæ‰©æ•£ç³»ç»Ÿçš„é«˜çº§æ¥å£ã€‚å®ƒå°†æ¨¡å‹ï¼ˆmodelï¼‰å’Œè°ƒåº¦å™¨ï¼ˆschedulerï¼‰ç­‰ç»„ä»¶æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥æ–¹ä¾¿åœ°è¿›è¡Œæ¨ç†å’Œå›¾åƒç”Ÿæˆã€‚pipelineé€šå¸¸åŒ…å«å¤šä¸ªç»„ä»¶ï¼Œå¦‚ç‰¹å¾æå–å™¨ã€å®‰å…¨æ£€æŸ¥å™¨ã€æ–‡æœ¬ç¼–ç å™¨ã€åˆ†è¯å™¨ã€UNetæ¨¡å‹ã€VAEæ¨¡å‹å’Œè°ƒåº¦å™¨ç­‰ã€‚
2. **Modelï¼ˆæ¨¡å‹ï¼‰**ï¼šmodelåœ¨æ‰©æ•£æ¨¡å‹ä¸­ä¸»è¦æŒ‡çš„æ˜¯UNetæ¨¡å‹ï¼ˆå¦‚UNet2DModelï¼‰å’ŒVAEæ¨¡å‹ï¼ˆå¦‚AutoencoderKLï¼‰ã€‚UNetè´Ÿè´£åœ¨æ¯ä¸ªæ—¶é—´æ­¥é¢„æµ‹å™ªå£°æ®‹å·®ï¼Œè€ŒVAEç”¨äºå°†å›¾åƒç¼–ç åˆ°æ½œåœ¨çš„ç©ºé—´å¹¶è¿›è¡Œè§£ç ã€‚è¿™äº›æ¨¡å‹æ˜¯æ‰§è¡Œæ‰©æ•£è¿‡ç¨‹çš„æ ¸å¿ƒï¼Œè´Ÿè´£ç”Ÿæˆå’Œå¤„ç†å›¾åƒæ•°æ®ã€‚
3. **Schedulerï¼ˆè°ƒåº¦å™¨ï¼‰**ï¼šscheduleræ§åˆ¶ç€æ‰©æ•£è¿‡ç¨‹ä¸­çš„æ—¶é—´æ­¥å’Œå™ªå£°è°ƒåº¦ã€‚å®ƒæ ¹æ®æ¨¡å‹é¢„æµ‹çš„å™ªå£°æ®‹å·®æ¥æ›´æ–°å›¾åƒï¼Œé€æ­¥ä»å™ªå£°å›¾åƒæ¢å¤å‡ºæ¸…æ™°çš„å›¾åƒã€‚ä¸åŒçš„schedulerå®ç°äº†ä¸åŒçš„æ‰©æ•£ç®—æ³•ï¼Œå¦‚DDPMã€DDIMã€PNDMç­‰ï¼Œå®ƒä»¬å†³å®šäº†å™ªå£°å¦‚ä½•è¢«æ·»åŠ å’Œå»é™¤ã€‚

**åŒºåˆ«ï¼š**

- pipelineæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„å·¥ä½œæµç¨‹ï¼Œæ–¹ä¾¿ç”¨æˆ·ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
- modelæ˜¯æ‰§è¡Œç”Ÿæˆä»»åŠ¡çš„å…·ä½“ç¥ç»ç½‘ç»œï¼Œè´Ÿè´£å›¾åƒçš„ç”Ÿæˆå’Œå¤„ç†ã€‚
- scheduleræ˜¯æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä¸­æ—¶é—´æ­¥å’Œå™ªå£°ç­–ç•¥çš„ç®—æ³•ï¼Œå®ƒä¸modelç´§å¯†åä½œï¼Œä½†æœ¬èº«ä¸åŒ…å«æ¨¡å‹æƒé‡ã€‚

**è”ç³»ï¼š**

- pipelineé€šå¸¸åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªmodelå’Œä¸€ä¸ªschedulerï¼Œå®ƒä»¬å…±åŒå·¥ä½œä»¥å®ç°å›¾åƒçš„ç”Ÿæˆã€‚
- modelçš„è¾“å‡ºä¾èµ–äºscheduleræä¾›çš„æ—¶é—´æ­¥ä¿¡æ¯ï¼Œè€Œschedulerçš„è¡Œä¸ºåˆ™ç”±modelçš„è¾“å‡ºæŒ‡å¯¼ã€‚
- ç”¨æˆ·å¯ä»¥æ ¹æ®éœ€è¦æ›´æ¢pipelineä¸­çš„scheduleræˆ–modelï¼Œä»¥é€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯æˆ–ä¼˜åŒ–æ€§èƒ½ã€‚

#### ä½¿ç”¨diffusersåˆ›å»ºè‡ªå®šä¹‰pipeline

åˆ›å»ºè‡ªå®šä¹‰pipelineæ˜¯`diffusers`çš„é«˜çº§ç”¨æ³•ï¼Œå¯ä»¥çµæ´»çš„æ›¿æ¢VAEæˆ–schedulerç­‰ç»„ä»¶ã€‚é¢„è®­ç»ƒæ¨¡å‹`stablediffusionapi/cyberrealistic-41`åŒ…å«ç»„æˆdiffusion pipelineçš„å®Œæ•´ç»„ä»¶ï¼Œå®ƒä»¬å­˜å‚¨åœ¨ä»¥ä¸‹æ–‡ä»¶å¤¹ä¸­ï¼š

![](../assets/images/Hung-yi_Lee/hw10-10.png)

- schedulerï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ­¥å‘å›¾åƒä¸­æ·»åŠ å™ªå£°ï¼›
- text_encoderï¼šå°†promptçš„tokenè½¬æ¢ä¸ºUNetå¯ä»¥ç†è§£çš„embeddingè¡¨ç¤ºï¼›
- tokenizerï¼šå°†è¾“å…¥çš„promptè½¬åŒ–ä¸ºtokenï¼›
- unetï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç”Ÿæˆå›¾åƒçš„æ½œåœ¨è¡¨ç¤ºçš„æ¨¡å‹ï¼›
- vaeï¼šautoencoderæ¨¡å—ï¼Œå°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºçœŸå®å›¾ç‰‡ã€‚

æˆ‘ä»¬å¯ä»¥å‘`from_pretrained()`æ–¹æ³•æŒ‡å®š`subfolder`å‚æ•°ä»æ–‡ä»¶å¤¹ä¸­åŠ è½½ç›¸åº”ç»„ä»¶ã€‚

```python
noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder="scheduler")
    
    # CLIPæ¨¡å‹çš„åˆ†è¯å™¨ï¼Œç”¨äºå°†æ–‡æœ¬å­—ç¬¦ä¸²è½¬æ¢ä¸ºtoken idåºåˆ—
    tokenizer = CLIPTokenizer.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="tokenizer"
    )

    # CLIPTextModelæ˜¯CLIPæ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨éƒ¨åˆ†ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥è¡¨ç¤º
    text_encoder = CLIPTextModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="text_encoder"
    )

    # AutoencoderKLæ˜¯ä¸€ä¸ªVAEæ¨¡å‹ï¼Œç”¨äºå°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´
    vae = AutoencoderKL.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="vae"
    )

    # UNet2DConditionModelæ˜¯ä¸€ä¸ªç”¨äºæ‰©æ•£æ¨¡å‹çš„U-Netæ¨¡å‹ï¼Œç”¨äºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é¢„æµ‹å™ªå£°
    unet = UNet2DConditionModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="unet"
    )
```

ä½¿ç”¨`peft`åº“çš„`get_peft_model()`å°†LoRAé›†æˆåˆ°Stable Diffusionçš„CLIPå’ŒU_netæ¨¡å—ã€‚

```python
# å°†LoRAé›†æˆåˆ°text_encoderå’Œunet
    text_encoder = get_peft_model(text_encoder, lora_config)
    unet = get_peft_model(unet, lora_config)
# æ‰“å°å¯è®­ç»ƒå‚æ•°
    text_encoder.print_trainable_parameters()
    unet.print_trainable_parameters()
```

trainable params: 2,359,296 || all params: 125,419,776 || trainable%: 1.8811 

trainable params: 6,377,472 || all params: 865,898,436 || trainable%: 0.7365

ä½¿ç”¨LoRAåï¼Œéœ€è®­ç»ƒçš„å‚æ•°ä¸åˆ°åŸæ¥çš„2%ã€‚

å®šä¹‰`prepare_lora_model()`å‡½æ•°å°è£…åŒ…å«LoRAå±‚çš„å®Œæ•´Stable Diffusionæ¨¡å‹ï¼š

```python
def prepare_lora_model(pretrained_model_name_or_path="runwayml/stable-diffusion-v1-5", model_path=None):
    """
    (1) Goal:
        - This function is used to get the whole stable diffusion model with lora layers and freeze non-lora parameters, including Tokenizer, Noise Scheduler, UNet, Text Encoder, and VAE

    (2) Arguments:
        - pretrained_model_name_or_path: str, model name from Hugging Face
        - model_path: str, path to pretrained model.

    (3) Returns:
        - output: Tokenizer, Noise Scheduler, UNet, Text Encoder, and VAE

    """
    noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_name_or_path, subfolder="scheduler")
    tokenizer = CLIPTokenizer.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="tokenizer"
    )
    text_encoder = CLIPTextModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="text_encoder"
    )
    vae = AutoencoderKL.from_pretrained(
        pretrained_model_name_or_path,
        subfolder="vae"
    )
    unet = UNet2DConditionModel.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        subfolder="unet"
    )

    # å°†LoRAé›†æˆåˆ°text_encoderå’Œunet
    text_encoder = get_peft_model(text_encoder, lora_config)
    unet = get_peft_model(unet, lora_config)

    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    text_encoder.print_trainable_parameters()
    unet.print_trainable_parameters()


    # text_encoder = torch.load(os.path.join(model_path, "text_encoder.pt"))
    # unet = torch.load(os.path.join(model_path, "unet.pt"))

    # å†»ç»“vaeå‚æ•°
    vae.requires_grad_(False)

    unet.to(DEVICE, dtype=weight_dtype)
    vae.to(DEVICE, dtype=weight_dtype)
    text_encoder.to(DEVICE, dtype=weight_dtype)
    return tokenizer, noise_scheduler, unet, vae, text_encoder
```

### å‡†å¤‡å¾®è°ƒæ‰€éœ€çš„æ•°æ®é›†ã€LoRAæ¨¡å‹å’Œä¼˜åŒ–å™¨

**åŠ è½½å¸¦LoRAå±‚çš„Stable Diffusionæ¨¡å‹**

```python
tokenizer, noise_scheduler, unet, vae, text_encoder = prepare_lora_model(pretrained_model_name_or_path, model_path)
```

**åˆ›å»ºä¼˜åŒ–å™¨**

```python
def prepare_optimizer(unet, text_encoder, unet_learning_rate=5e-4, text_encoder_learning_rate=1e-4):
    """
    (1) Goal:
        - This function is used to feed trainable parameters from UNet and Text Encoder in to optimizer each with different learning rate

    (2) Arguments:
        - unet: UNet2DConditionModel, UNet from Hugging Face
        - text_encoder: CLIPTextModel, Text Encoder from Hugging Face
        - unet_learning_rate: float, learning rate for UNet
        - text_encoder_learning_rate: float, learning rate for Text Encoder

    (3) Returns:
        - output: Optimizer

    """
    # ç­›é€‰UNetæ¨¡å‹ä¸­éœ€è¦æ¢¯åº¦çš„å‚æ•°
    unet_lora_layers = list(filter(lambda p: p.requires_grad, unet.parameters()))
    # ç­›é€‰text_encoderä¸­éœ€è¦æ¢¯åº¦çš„å‚æ•°
    text_encoder_lora_layers = list(filter(lambda p: p.requires_grad, text_encoder.parameters()))

    # é…ç½®å¯è®­ç»ƒå‚æ•°åˆ—è¡¨
    trainable_params = [
        {"params": unet_lora_layers, "lr": unet_learning_rate},
        {"params": text_encoder_lora_layers, "lr": text_encoder_learning_rate}
    ]

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        trainable_params,
        lr=unet_learning_rate,
    )
    return optimizer

optimizer = prepare_optimizer(
    unet,
    text_encoder,
    unet_learning_rate,
    text_encoder_learning_rate
)
```

**åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨**

```python
lr_scheduler = get_scheduler(
    lr_scheduler_name,
    optimizer=optimizer,
    num_warmup_steps=lr_warmup_steps,
    num_training_steps=max_train_steps,
    num_cycles=3
)
```

**å‡†å¤‡æ•°æ®é›†**

```python
class Text2ImageDataset(torch.utils.data.Dataset):
    """
    (1) Goal:
        - This class is used to build dataset for finetuning text-to-image model

    """
    def __init__(self, images_folder, captions_folder, transform, tokenizer):
        """
        (2) Arguments:
            - images_folder: str, path to images
            - captions_folder: str, path to captions
            - transform: function, turn raw image into torch.tensor
            - tokenizer: CLIPTokenize, turn sentences into word ids
        """
        self.image_paths = []
        for ext in IMAGE_EXTENSIONS:
            self.image_paths.extend(glob.glob(f"{images_folder}/*{ext}"))
        self.image_paths = sorted(self.image_paths)

        # éå†å›¾åƒè·¯å¾„ï¼Œä½¿ç”¨DeepFaceæå–é¢éƒ¨ç‰¹å¾åµŒå…¥
        self.train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend="ssd", model_name="GhostFaceNet", enforce_detection=False)[0]['embedding'] for img_path in self.image_paths])
        caption_paths = sorted(glob.glob(f"{captions_folder}/*txt"))
        captions = []
        for p in caption_paths:
            with open(p, "r") as f:
                captions.append(f.readline())
        # å°†æ–‡æœ¬è½¬åŒ–ä¸ºtoken
        inputs = tokenizer(
            captions, max_length=tokenizer.model_max_length, padding="max_length", truncation=True, return_tensors="pt"
        )
        self.input_ids = inputs.input_ids
        self.transform = transform

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        input_id = self.input_ids[idx]
        try:
            image = Image.open(img_path).convert("RGB")
            # convert to tensor temporarily so dataloader will accept it
            tensor = self.transform(image)
        except Exception as e:
            print(f"Could not load image path: {img_path}, error: {e}")
            return None


        return tensor, input_id

    def __len__(self):
        return len(self.image_paths)

dataset = Text2ImageDataset(
    images_folder=images_folder,
    captions_folder=captions_folder,
    transform=train_transform,
    tokenizer=tokenizer,
)

# è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œå°†å¤šä¸ªæ ·æœ¬ï¼ˆexamplesï¼‰ç»„åˆæˆä¸€ä¸ªbatch
def collate_fn(examples):
    pixel_values = []
    input_ids = []
    for tensor, input_id in examples:
        pixel_values.append(tensor)
        input_ids.append(input_id)

    # å›¾åƒtensorå †å æˆä¸€ä¸ªå¤šç»´tensor
    pixel_values = torch.stack(pixel_values, dim=0).float()
    # input_idså †å æˆä¸€ä¸ªå¤šç»´tensor
    input_ids = torch.stack(input_ids, dim=0)
    return {"pixel_values": pixel_values, "input_ids": input_ids}

# ä½¿ç”¨pytorch DataLoaderåŠ è½½æ•°æ®é›†
train_dataloader = torch.utils.data.DataLoader(
    dataset,
    shuffle=True,
    collate_fn=collate_fn,
    batch_size=train_batch_size,
    num_workers=8,
)
```

### å¼€å§‹å¾®è°ƒ

è®­ç»ƒçš„æŸå¤±å‡½æ•°é‡‡ç”¨[Min-SNR](https://arxiv.org/abs/2303.09556)ï¼ˆæœ€å°ä¿¡å™ªæ¯”åŠ æƒï¼‰ç­–ç•¥ï¼Œä»¥åŠ å¿«æ‰©æ•£æ¨¡å‹æ”¶æ•›ã€‚

Denoising diffusion modelæ˜¯å›¾åƒç”Ÿæˆçš„ä¸»æµæ–¹æ³•ï¼Œç„¶è€Œï¼Œè®­ç»ƒè¿™äº›æ¨¡å‹å¾€å¾€å­˜åœ¨æ”¶æ•›ç¼“æ…¢çš„é—®é¢˜ã€‚Hangç­‰äººå‘ç°æ”¶æ•›ç¼“æ…¢çš„éƒ¨åˆ†åŸå› æ˜¯ç”±äºæ—¶é—´æ­¥é—´ä¼˜åŒ–æ–¹å‘å†²çªå¯¼è‡´çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä»–ä»¬å°†æ‰©æ•£æ¨¡å‹çš„è®­ç»ƒè§†ä¸ºå¤šä»»åŠ¡å­¦ä¹ ï¼ˆmulti-task learningï¼‰é—®é¢˜ï¼Œå¼•å…¥$Min-SNR-\gamma$æ–¹æ³•ã€‚è¯¥æ–¹æ³•æ›´å…·è¢«é™åˆ¶çš„ä¿¡å™ªæ¯”è°ƒæ•´æ—¶é—´æ­¥çš„æŸå¤±æƒé‡ï¼Œæœ‰æ•ˆåœ°å¹³è¡¡äº†æ—¶é—´æ­¥ä¹‹é—´çš„å†²çªï¼Œæ”¶æ•›é€Ÿåº¦æ¯”ä¼ ç»Ÿæ–¹æ³•å¿«3.4å€ã€‚

å¾®è°ƒæ¨¡å‹å¹¶ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹ï¼Œè®­ç»ƒå¾ªç¯å¦‚ä¸‹ï¼š

```python
global_step = 0
num_epochs = math.ceil(max_train_steps / len(train_dataloader))
validation_step = int(max_train_steps * validation_step_ratio)
best_face_score = float("inf")
for epoch in range(num_epochs):
    torch.cuda.empty_cache()
    unet.train()
    text_encoder.train()
    for step, batch in enumerate(train_dataloader):
        if global_step >= max_train_steps:
            break

        # ä½¿ç”¨vaeå°†å›¾åƒç¼–ç ä¸ºlatent representation
        latents = vae.encode(batch["pixel_values"].to(DEVICE, dtype=weight_dtype)).latent_dist.sample()
        latents = latents * vae.config.scaling_factor

        # Sample noise that we'll add to the latents
        noise = torch.randn_like(latents)
        bsz = latents.shape[0]
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)
        timesteps = timesteps.long()
        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

        # Get the text embedding for conditioning
        encoder_hidden_states = text_encoder(batch["input_ids"].to(latents.device), return_dict=False)[0]
        if noise_scheduler.config.prediction_type == "epsilon":
            target = noise
        elif noise_scheduler.config.prediction_type == "v_prediction":
            target = noise_scheduler.get_velocity(latents, noise, timesteps)

        # è¾“å…¥å™ªå£°ã€æ—¶é—´æ­¥ã€text embeddingï¼Œä½¿ç”¨unetè¿›è¡Œé¢„æµ‹
        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]
        if not snr_gamma: # ä¸ä½¿ç”¨snr_gamma
            loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean") # æ ‡å‡†å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
        else: # ä½¿ç”¨snr_gammaï¼ŒåŸºäºä¿¡å™ªæ¯”å¯¹æŸå¤±è¿›è¡ŒåŠ æƒ
            snr = compute_snr(noise_scheduler, timesteps) #è®¡ç®—ç»™å®štimestepçš„snrå€¼
            mse_loss_weights = torch.stack([snr, snr_gamma * torch.ones_like(timesteps)], dim=1).min(
                dim=1
            )[0] #ä½¿ç”¨snrå’Œsnr_gammaè®¡ç®—æ¯ä¸ªtimestepçš„æŸå¤±æƒé‡
            if noise_scheduler.config.prediction_type == "epsilon": # å™ªå£°é¢„æµ‹
                mse_loss_weights = mse_loss_weights / snr # é™ä½æƒé‡éšä¿¡å™ªæ¯”çš„å˜åŒ–
            elif noise_scheduler.config.prediction_type == "v_prediction": # é€Ÿåº¦é¢„æµ‹
                mse_loss_weights = mse_loss_weights / (snr + 1) # # è¿›ä¸€æ­¥å¹³æ»‘è°ƒæ•´æƒé‡

            loss = F.mse_loss(model_pred.float(), target.float(), reduction="none") # è®¡ç®—é€å…ƒç´ çš„å‡æ–¹è¯¯å·®
            loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights # å¯¹éæ‰¹æ¬¡ç»´åº¦å–å‡å€¼ï¼Œå¾—åˆ°æ¯ä¸ªæ ·æœ¬çš„æŸå¤±ï¼Œä¹˜ä»¥å¯¹åº”æ—¶é—´æ­¥çš„åŠ æƒå› å­
            loss = loss.mean() # å¯¹æ‰€æœ‰æ ·æœ¬çš„åŠ æƒæŸå¤±æ±‚å¹³å‡å€¼ï¼Œä½œä¸ºæœ€ç»ˆæŸå¤±å€¼
        
        # åå‘ä¼ æ’­
        loss.backward() # è®¡ç®—æ¢¯åº¦
        optimizer.step() # æ›´æ–°æ¨¡å‹å‚æ•°
        lr_scheduler.step() # æ›´æ–°å­¦ä¹ ç‡
        optimizer.zero_grad() # æ¢¯åº¦æ¸…é›¶
        progress_bar.update(1) # æ›´æ–°è¿›åº¦æ¡
        global_step += 1 # æ›´æ–°å…¨å±€æ­¥æ•°

        # éªŒè¯æ¨¡å‹æ€§èƒ½
        if global_step % validation_step == 0 or global_step == max_train_steps:
            # ä¿å­˜å½“å‰æ£€æŸ¥ç‚¹
            save_path = os.path.join(output_folder, f"checkpoint-last")
            unet_path = os.path.join(save_path, "unet.pt")
            text_encoder_path = os.path.join(save_path, "text_encoder.pt")
            print(f"Saving Checkpoint to {save_path} ......")
            os.makedirs(save_path, exist_ok=True)
            torch.save(unet, unet_path)
            torch.save(text_encoder, text_encoder_path)
            save_path = os.path.join(output_folder, f"checkpoint-{global_step + 1000}")
            os.makedirs(save_path, exist_ok=True)

            # è¯„ä¼°æ¨¡å‹æ€§èƒ½
            face_score, clip_score, mis = evaluate(
                pretrained_model_name_or_path=pretrained_model_name_or_path,
                weight_dtype=weight_dtype,
                seed=seed,
                unet_path=unet_path,
                text_encoder_path=text_encoder_path,
                validation_prompt=validation_prompt[:validation_prompt_num],
                output_folder=save_path,
                train_emb=dataset.train_emb
            )
            print("Step:", global_step, "Face Similarity Score:", face_score, "CLIP Score:", clip_score, "Faceless Images:", mis)
            if face_score < best_face_score: # ä¿å­˜å½“å‰æœ€ä½³ç»“æœ
                best_face_score = face_score
                save_path = os.path.join(output_folder, f"checkpoint-best")
                unet_path = os.path.join(save_path, "unet.pt")
                text_encoder_path = os.path.join(save_path, "text_encoder.pt")
                os.makedirs(save_path, exist_ok=True)
                torch.save(unet, unet_path)
                torch.save(text_encoder, text_encoder_path)
print("Fine-tuning Finished!!!")
```

## Step 2. Generate Images

ä½¿ç”¨éªŒè¯é›†promptå’Œfine tuneæœ€ç»ˆå¾—åˆ°çš„æ¨¡å‹ç”Ÿæˆå›¾ç‰‡ï¼Œç”¨äºéªŒè¯çš„promptå¦‚ä¸‹ï¼š

```tex
A man in a black hoodie and khaki pants.
A man sports a red polo and denim jacket.
A man wears a blue shirt and brown blazer.
A man dons a white button-up and cardigan.
A man in a striped shirt and leather jacket.
A man wears a green sweater and gray vest.
A man sports a black suit and tie.
A man in a denim shirt and bomber jacket.
A man wears a plaid flannel and puffer vest.
A man dons a hoodie and windbreaker.
A man in a V-neck sweater and coat.
A man wears a checkered shirt and trench coat.
A man in a graphic tee and sport coat.
A man sports a hoodie and quilted jacket.
A man wears a button-up and suede jacket.
A man in a knit sweater and leather vest.
A man dons a pullover and duffle coat.
A man wears a henley shirt and parka.
A man in a zip-up hoodie and blazer.
A man sports a chambray shirt and overcoat.
A man wears a crewneck sweater and bomber.
A man in a long-sleeve tee and pea coat.
A man dons a polo shirt and varsity jacket.
A man wears a patterned shirt and raincoat.
A man in a mock neck and moto jacket.
```

åŠ è½½æœ€åä¸€ä¸ªæ£€æŸ¥ç‚¹`checkpoint-last`ä½œä¸ºæœ€ç»ˆçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚ä½¿ç”¨`def evaluate()`å‡½æ•°ç”Ÿæˆå›¾ç‰‡å’Œè¯„åˆ†ã€‚

```python
torch.cuda.empty_cache()
checkpoint_path = os.path.join(output_folder, f"checkpoint-last") # è¨­å®šä½¿ç”¨å“ªå€‹checkpoint inference
unet_path = os.path.join(checkpoint_path, "unet.pt")
text_encoder_path = os.path.join(checkpoint_path, "text_encoder.pt")
inference_path = os.path.join(project_dir, "inference")
os.makedirs(inference_path, exist_ok=True)
train_image_paths = []
for ext in IMAGE_EXTENSIONS:
    train_image_paths.extend(glob.glob(f"{images_folder}/*{ext}"))
train_image_paths = sorted(train_image_paths)
train_emb = torch.tensor([DeepFace.represent(img_path, detector_backend="ssd", model_name="GhostFaceNet", enforce_detection=False)[0]['embedding'] for img_path in train_image_paths])

face_score, clip_score, mis = evaluate(
    pretrained_model_name_or_path=pretrained_model_name_or_path,
    weight_dtype=weight_dtype,
    seed=seed,
    unet_path=unet_path,
    text_encoder_path=text_encoder_path,
    validation_prompt=validation_prompt,
    output_folder=inference_path,
    train_emb=train_emb,
)
print("Face Similarity Score:", face_score, "CLIP Score:", clip_score, "Faceless Images:", mis)
```

æœ€ç»ˆç”Ÿæˆçš„å›¾ç‰‡éƒ½æ˜¯Brad Pittçš„è„¸é…ä¸Šä¸åŒçš„è¡£æœã€‚

![](../assets/images/Hung-yi_Lee/hw10-11.png)

## Step 3. Evaluate Images

### Face Distance Score

å¯¹äºæ¯å¼ ç”Ÿæˆçš„äººè„¸ï¼Œè®¡ç®—å®ƒä¸æ‰€æœ‰è®­ç»ƒå›¾ç‰‡çš„å¹³å‡è·ç¦»ï¼Œç„¶åå¯¹æ‰€æœ‰ç”Ÿæˆçš„å›¾ç‰‡å–å¹³å‡ã€‚äººè„¸çš„è·ç¦»ä½¿ç”¨ç¥ç»ç½‘ç»œ[GhostFaceNet](https://github.com/HamadYA/GhostFaceNets)è®¡ç®—ã€‚
$$
F(D_G,D_T) = \frac{1}{\Vert D_G \Vert \Vert D_T \Vert}\sum_{d_G \in D_G}\sum_{d_T \in D_T}f(d_G,d_T)
$$
å…¶ä¸­ï¼Œ$D_G$æ˜¯ç”Ÿæˆçš„äººè„¸ï¼Œ$D_T$æ˜¯è®­ç»ƒæ•°æ®ï¼Œ$f(*)$æ˜¯GhostFaceNetã€‚

æˆ‘ä»¬è¦ç”ŸæˆBrad Pittçš„è„¸ï¼Œå› æ­¤$F(D_G,D_T)$è¶Šå°è¶Šå¥½ã€‚

### CLIP Score

[CLIP](https://github.com/openai/CLIP) Scoreå¯ä»¥è¡¡é‡æ–‡æœ¬å’Œå›¾åƒä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå¾—åˆ†è¶Šé«˜ç›¸å…³æ€§è¶Šå¤§ã€‚CLIPæ¨¡å‹ä½¿ç”¨`openai/clip-vit-base-patch32`ã€‚

å®šä¹‰è¯„ä¼°å‡½æ•°ï¼Œè®¡ç®—å›¾ç‰‡çš„ç›¸ä¼¼æ€§ä»¥åŠæ–‡æœ¬-å›¾åƒçš„åŒ¹é…ç¨‹åº¦ã€‚

```python
def evaluate(pretrained_model_name_or_path, weight_dtype, seed, unet_path, text_encoder_path, validation_prompt, output_folder, train_emb):
    """
    (1) Goal:
        - This function is used to evaluate Stable Diffusion by loading UNet and Text Encoder from the given path and calculating face similarity, CLIP score, and the number of faceless images.

    (2) Arguments:
        - pretrained_model_name_or_path: str, model name from Hugging Face
        - weight_dtype: torch.type, model weight type
        - seed: int, random seed
        - unet_path: str, path to UNet model checkpoint
        - text_encoder_path: str, path to Text Encoder model checkpoint
        - validation_prompt: list, list of str storing texts for validation
        - output_folder: str, directory for saving generated images
        - train_emb: tensor, face features of training images

    (3) Returns:
        - output: face similarity, CLIP score, the number of faceless images

    """
    pipeline = DiffusionPipeline.from_pretrained(
        pretrained_model_name_or_path,
        torch_dtype=weight_dtype,
        safety_checker=None,
    )
    pipeline.unet = torch.load(unet_path)
    pipeline.text_encoder = torch.load(text_encoder_path)
    pipeline = pipeline.to(DEVICE)
    clip_model_name = "openai/clip-vit-base-patch32"
    clip_model = AutoModel.from_pretrained(clip_model_name)
    clip_processor = AutoProcessor.from_pretrained(clip_model_name)

    # run inference
    with torch.no_grad():
        generator = torch.Generator(device=DEVICE) # åˆ›å»ºä¸€ä¸ªæ–°çš„ä¼ªéšæœºæ•°ç”Ÿæˆå™¨
        generator = generator.manual_seed(seed) # è®¾ç½®éšæœºæ•°ç§å­
        face_score = 0
        clip_score = 0
        mis = 0
        print("Generating validaion pictures ......")
        images = []
        for i in range(0, len(validation_prompt), 4): # éå†validation_promptï¼Œæ¯æ¬¡å¤„ç†4ä¸ªæç¤º
            # ä½¿ç”¨pipelineæ ¹æ®æç¤ºç”Ÿæˆå›¾åƒï¼Œæ·»åŠ åˆ°imagesåˆ—è¡¨ä¸­
            images.extend(pipeline(validation_prompt[i:min(i + 4, len(validation_prompt))], num_inference_steps=30, generator=generator).images)
        
        # è®¡ç®—é¢éƒ¨ç›¸ä¼¼åº¦å’ŒCLIPåˆ†æ•°
        print("Calculating validaion score ......")
        valid_emb = []
        for i, image in enumerate(tqdm(images)):
            torch.cuda.empty_cache()
            save_file = f"{output_folder}/valid_image_{i}.png"
            image.save(save_file) # å°†ç”Ÿæˆçš„å›¾åƒä¿å­˜åˆ°æŒ‡å®šç›®å½•
            opencvImage = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR) # ä½¿ç”¨OpenCVçš„cv2Colorå‡½æ•°å°†å›¾åƒä»RGBé¢œè‰²ç©ºé—´è½¬åŒ–ä¸ºBGRé¢œè‰²ç©ºé—´
            emb = DeepFace.represent( # ä½¿ç”¨DeepFaceåº“æå–é¢éƒ¨ç‰¹å¾åµŒå…¥
                opencvImage,
                detector_backend="ssd",
                model_name="GhostFaceNet",
                enforce_detection=False,
            )
            if emb == [] or emb[0]['face_confidence'] == 0: # ç»Ÿè®¡æ— æ³•æ£€æµ‹åˆ°é¢éƒ¨çš„å›¾ç‰‡
                mis += 1
                continue
            # è®¡ç®—CLIPåˆ†æ•°ï¼Œè¯„ä¼°å›¾ç‰‡ä¸æ–‡æœ¬çš„åŒ¹é…ç¨‹åº¦
            emb = emb[0]
            inputs = clip_processor(text=validation_prompt[i], images=image, return_tensors="pt") # å¤„ç†CLIPæ¨¡å‹çš„è¾“å…¥ï¼Œè¾“å…¥æ–‡æœ¬å’Œå›¾åƒï¼Œè¿”å›é€‚åˆæ¨¡å‹è¾“å…¥çš„æ ¼å¼
            with torch.no_grad():
                outputs = clip_model(**inputs) # ä½¿ç”¨CLIPæ¨¡å‹ï¼Œè®¡ç®—æ–‡æœ¬å’Œå›¾åƒçš„ç›¸ä¼¼åº¦
            sim = outputs.logits_per_image # æå–ç›¸ä¼¼åº¦åˆ†æ•°
            clip_score += sim.item() # tensorè½¬ä¸ºæ•°å€¼
            valid_emb.append(emb['embedding'])
        if len(valid_emb) == 0:
            return 0, 0, mis
        valid_emb = torch.tensor(valid_emb)
        valid_emb = (valid_emb / torch.norm(valid_emb, p=2, dim=-1)[:, None]).cuda() # å½’ä¸€åŒ–å¤„ç†
        train_emb = (train_emb / torch.norm(train_emb, p=2, dim=-1)[:, None]).cuda()
        face_score = torch.cdist(valid_emb, train_emb, p=2).mean().item() # è®¡ç®—è·ç¦»
        # face_score = torch.min(face_score, 1)[0].mean()
        clip_score /= len(validation_prompt) - mis # è®¡ç®—å¹³å‡ç›¸ä¼¼åº¦
    return face_score, clip_score, mis
```

## Results

**è®­ç»ƒ200ä¸ªstep:**

Step: 200 Face Similarity Score: 1.1819632053375244 CLIP Score: 30.577381134033203 Faceless Images: 0

Face Similarity Score: 1.2155983448028564 CLIP Score: 30.146756172180176 Faceless Images: 1

**è®­ç»ƒ2000ä¸ªstep:** ç»“æœç•¥å¾®æå‡ï¼Œä½†ä¸æ˜æ˜¾

Step: 2000 Face Similarity Score: 1.1477864980697632 CLIP Score: 30.112869262695312 Faceless Images: 0

Face Similarity Score: 1.1696956157684326 CLIP Score: 29.713413848876954 Faceless Images: 0

## Link

[å®Œæ•´ä»£ç ](https://colab.research.google.com/drive/1ue6knQcAEJB3kTv8DqSq-nBCYv-gES2d#scrollTo=3Kuc0_PcHW48)

## Reference

1. [Stable Diffusion with ğŸ§¨ Diffusers](https://huggingface.co/blog/stable_diffusion#stable-diffusion-with-%F0%9F%A7%A8-diffusers)

2. [Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=aCH4p1dtyaXX)

3. [Training with Diffusers](https://colab.research.google.com/gist/anton-l/f3a8206dae4125b93f05b1f5f703191d/diffusers_training_example.ipynb)

4. [Understanding pipelines, models and schedulers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers_doc/en/pytorch/write_own_pipeline.ipynb#scrollTo=SwW8Va1frhDF)

5. [Efficient Diffusion Training via Min-SNR Weighting Strategy](https://arxiv.org/abs/2303.09556)

   

